<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.8.27">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>6&nbsp; Conditional Probability and Bayes’ Rule – Mathematics of Poker</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../site_libs/quarto-search/fuse.min.js"></script>
<script src="../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../">
<link href="../chapters/06-expected-value-in-multi-stage-decisions.html" rel="next">
<link href="../chapters/04-random-variables-expectation-and-variance.html" rel="prev">
<script src="../site_libs/quarto-html/quarto.js" type="module"></script>
<script src="../site_libs/quarto-html/tabsets/tabsets.js" type="module"></script>
<script src="../site_libs/quarto-html/axe/axe-check.js" type="module"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../site_libs/quarto-html/anchor.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-ed96de9b727972fe78a7b5d16c58bf87.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../site_libs/bootstrap/bootstrap-d9ea66fcd6c317012447f26e9c021348.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>

  <script>window.backupDefine = window.define; window.define = undefined;</script><script src="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.js"></script>
  <script>document.addEventListener("DOMContentLoaded", function () {
 var mathElements = document.getElementsByClassName("math");
 var macros = [];
 for (var i = 0; i < mathElements.length; i++) {
  var texText = mathElements[i].firstChild;
  if (mathElements[i].tagName == "SPAN") {
   katex.render(texText.data, mathElements[i], {
    displayMode: mathElements[i].classList.contains('display'),
    throwOnError: false,
    macros: macros,
    fleqn: false
   });
}}});
  </script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@latest/dist/katex.min.css">

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN" && texText && texText.data) {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="nav-sidebar floating quarto-light">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="../chapters/05-conditional-probability-and-bayes-rule.html"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Rule</span></a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation floating overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="../">Mathematics of Poker</a> 
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../index.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Mathematics of Poker</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/01-poker-games-structure-and-language.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Poker Games, Structure, and Language</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/02-counting-hands-and-boards.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Counting Hands and Boards</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/03-poker-as-a-probability-space.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Poker as a Probability Space</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/04-random-variables-expectation-and-variance.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Random Variables, Expectation, and Variance</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/05-conditional-probability-and-bayes-rule.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Rule</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/06-expected-value-in-multi-stage-decisions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Expected Value in Multi-Stage Decisions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/07-inference-with-discrete-distributions-ranges-and-blockers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">8</span>&nbsp; <span class="chapter-title">Inference with Discrete Distributions: Ranges and Blockers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/08-zero-sum-games-and-mixed-strategies.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">9</span>&nbsp; <span class="chapter-title">Motivating Example</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/09-nash-equilibrium-and-indifference-conditions.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">10</span>&nbsp; <span class="chapter-title">Nash Equilibrium and Indifference Conditions</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/10-exploitative-play-vs-equilibrium-thinking.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">11</span>&nbsp; <span class="chapter-title">Exploitative Play vs Equilibrium Thinking</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/11-betting-as-optimization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">12</span>&nbsp; <span class="chapter-title">Betting as Optimization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/12-the-value-of-information.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">13</span>&nbsp; <span class="chapter-title">The Value of Information</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/13-tournament-poker-and-icm-optimization-with-nonlinear-utility.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">14</span>&nbsp; <span class="chapter-title">Tournament Poker and ICM: Optimization with Nonlinear Utility</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/14-game-trees-and-abstraction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">15</span>&nbsp; <span class="chapter-title">Test</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/15-learning-equilibria-regret-minimization-and-solvers.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">16</span>&nbsp; <span class="chapter-title">Learning Equilibria: Regret Minimization and Solvers</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/16-alternative-solver-perspectives-lp-qre-and-regularization.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">17</span>&nbsp; <span class="chapter-title">Alternative Solver Perspectives: LP, QRE, and Regularization</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/17-variance-bankroll-and-risk-of-ruin.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">18</span>&nbsp; <span class="chapter-title">Variance, Bankroll, and Risk of Ruin</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/18-randomness-shuffling-rngs-and-verification.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">19</span>&nbsp; <span class="chapter-title">Randomness, Shuffling, RNGs, and Verification</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/19-learning-from-data-hand-histories-and-statistical-inference.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">20</span>&nbsp; <span class="chapter-title">Learning from Data: Hand Histories and Statistical Inference</span></span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="../chapters/20-detecting-cheating-and-collusion.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text"><span class="chapter-number">21</span>&nbsp; <span class="chapter-title">Detecting Cheating and Collusion</span></span></a>
  </div>
</li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#motivating-example-what-does-a-bet-tell-you" id="toc-motivating-example-what-does-a-bet-tell-you" class="nav-link active" data-scroll-target="#motivating-example-what-does-a-bet-tell-you"><span class="header-section-number">6.1</span> Motivating Example: What Does a Bet Tell You?</a>
  <ul class="collapse">
  <li><a href="#the-puzzle" id="toc-the-puzzle" class="nav-link" data-scroll-target="#the-puzzle"><span class="header-section-number">6.1.1</span> The Puzzle</a></li>
  <li><a href="#why-naive-reasoning-fails" id="toc-why-naive-reasoning-fails" class="nav-link" data-scroll-target="#why-naive-reasoning-fails"><span class="header-section-number">6.1.2</span> Why Naive Reasoning Fails</a></li>
  <li><a href="#hidden-states-and-observable-evidence" id="toc-hidden-states-and-observable-evidence" class="nav-link" data-scroll-target="#hidden-states-and-observable-evidence"><span class="header-section-number">6.1.3</span> Hidden States and Observable Evidence</a></li>
  <li><a href="#the-need-for-conditional-probability" id="toc-the-need-for-conditional-probability" class="nav-link" data-scroll-target="#the-need-for-conditional-probability"><span class="header-section-number">6.1.4</span> The Need for Conditional Probability</a></li>
  </ul></li>
  <li><a href="#mathematical-framework-and-poker-theory" id="toc-mathematical-framework-and-poker-theory" class="nav-link" data-scroll-target="#mathematical-framework-and-poker-theory"><span class="header-section-number">6.2</span> Mathematical Framework and Poker Theory</a></li>
  <li><a href="#discrete-probability" id="toc-discrete-probability" class="nav-link" data-scroll-target="#discrete-probability"><span class="header-section-number">6.3</span> Discrete Probability</a>
  <ul class="collapse">
  <li><a href="#conditional-joint-and-total-probability" id="toc-conditional-joint-and-total-probability" class="nav-link" data-scroll-target="#conditional-joint-and-total-probability"><span class="header-section-number">6.3.1</span> Conditional, Joint, and Total Probability</a></li>
  <li><a href="#marginal-distributions-from-the-joint-distribution" id="toc-marginal-distributions-from-the-joint-distribution" class="nav-link" data-scroll-target="#marginal-distributions-from-the-joint-distribution"><span class="header-section-number">6.3.2</span> Marginal Distributions from the Joint Distribution</a></li>
  <li><a href="#computing-the-marginals-in-running-example-1" id="toc-computing-the-marginals-in-running-example-1" class="nav-link" data-scroll-target="#computing-the-marginals-in-running-example-1"><span class="header-section-number">6.3.3</span> Computing the Marginals in Running Example 1</a></li>
  <li><a href="#conditioning-on-a-bet-in-running-example-1" id="toc-conditioning-on-a-bet-in-running-example-1" class="nav-link" data-scroll-target="#conditioning-on-a-bet-in-running-example-1"><span class="header-section-number">6.3.4</span> Conditioning on a Bet in Running Example 1</a></li>
  <li><a href="#the-conditional-distribution-of-h-given-a-bet" id="toc-the-conditional-distribution-of-h-given-a-bet" class="nav-link" data-scroll-target="#the-conditional-distribution-of-h-given-a-bet"><span class="header-section-number">6.3.5</span> The Conditional Distribution of <span class="math inline">H</span> Given a Bet</a></li>
  </ul></li>
  <li><a href="#the-opponents-hand-has-not-changed.-only-our-information-about-it-has." id="toc-the-opponents-hand-has-not-changed.-only-our-information-about-it-has." class="nav-link" data-scroll-target="#the-opponents-hand-has-not-changed.-only-our-information-about-it-has."><span class="header-section-number">6.4</span> The opponent’s hand has not changed. Only our information about it has.</a>
  <ul class="collapse">
  <li><a href="#bayes-theorem-discrete" id="toc-bayes-theorem-discrete" class="nav-link" data-scroll-target="#bayes-theorem-discrete"><span class="header-section-number">6.4.1</span> Bayes’ Theorem (Discrete)</a></li>
  <li><a href="#step-1-extract-priors-and-likelihoods-from-the-table" id="toc-step-1-extract-priors-and-likelihoods-from-the-table" class="nav-link" data-scroll-target="#step-1-extract-priors-and-likelihoods-from-the-table"><span class="header-section-number">6.4.2</span> Step 1: Extract priors and likelihoods from the table</a></li>
  <li><a href="#step-2-compute-the-evidence-term-mathbbpatextbet" id="toc-step-2-compute-the-evidence-term-mathbbpatextbet" class="nav-link" data-scroll-target="#step-2-compute-the-evidence-term-mathbbpatextbet"><span class="header-section-number">6.4.3</span> Step 2: Compute the evidence term <span class="math inline">\mathbb{P}(A=\text{bet})</span></a></li>
  <li><a href="#step-3-compute-the-posterior-distribution" id="toc-step-3-compute-the-posterior-distribution" class="nav-link" data-scroll-target="#step-3-compute-the-posterior-distribution"><span class="header-section-number">6.4.4</span> Step 3: Compute the posterior distribution</a></li>
  <li><a href="#hidden-versus-observable-random-variables-discrete" id="toc-hidden-versus-observable-random-variables-discrete" class="nav-link" data-scroll-target="#hidden-versus-observable-random-variables-discrete"><span class="header-section-number">6.4.5</span> Hidden Versus Observable Random Variables (Discrete)</a></li>
  <li><a href="#transition-why-move-to-continuous-models" id="toc-transition-why-move-to-continuous-models" class="nav-link" data-scroll-target="#transition-why-move-to-continuous-models"><span class="header-section-number">6.4.6</span> Transition: Why Move to Continuous Models?</a></li>
  </ul></li>
  <li><a href="#continuous-probability" id="toc-continuous-probability" class="nav-link" data-scroll-target="#continuous-probability"><span class="header-section-number">6.5</span> Continuous Probability</a></li>
  <li><a href="#from-discrete-to-continuous-equity-and-bet-size" id="toc-from-discrete-to-continuous-equity-and-bet-size" class="nav-link" data-scroll-target="#from-discrete-to-continuous-equity-and-bet-size"><span class="header-section-number">6.6</span> From Discrete to Continuous: Equity and Bet Size</a>
  <ul class="collapse">
  <li><a href="#definition-equity-of-a-hand" id="toc-definition-equity-of-a-hand" class="nav-link" data-scroll-target="#definition-equity-of-a-hand"><span class="header-section-number">6.6.1</span> Definition: Equity of a Hand</a></li>
  <li><a href="#bet-size-as-a-continuous-observable" id="toc-bet-size-as-a-continuous-observable" class="nav-link" data-scroll-target="#bet-size-as-a-continuous-observable"><span class="header-section-number">6.6.2</span> Bet Size as a Continuous Observable</a></li>
  <li><a href="#joint-distribution-of-equity-and-bet-size" id="toc-joint-distribution-of-equity-and-bet-size" class="nav-link" data-scroll-target="#joint-distribution-of-equity-and-bet-size"><span class="header-section-number">6.6.3</span> Joint Distribution of Equity and Bet Size</a></li>
  <li><a href="#a-simple-joint-density-for-equity-and-bet-size" id="toc-a-simple-joint-density-for-equity-and-bet-size" class="nav-link" data-scroll-target="#a-simple-joint-density-for-equity-and-bet-size"><span class="header-section-number">6.6.4</span> A Simple Joint Density for Equity and Bet Size</a></li>
  <li><a href="#modeling-setup" id="toc-modeling-setup" class="nav-link" data-scroll-target="#modeling-setup"><span class="header-section-number">6.6.5</span> Modeling Setup</a></li>
  <li><a href="#definition-a-parametric-joint-density" id="toc-definition-a-parametric-joint-density" class="nav-link" data-scroll-target="#definition-a-parametric-joint-density"><span class="header-section-number">6.6.6</span> Definition: A Parametric Joint Density</a></li>
  <li><a href="#why-this-is-a-valid-density" id="toc-why-this-is-a-valid-density" class="nav-link" data-scroll-target="#why-this-is-a-valid-density"><span class="header-section-number">6.6.7</span> Why This Is a Valid Density</a></li>
  <li><a href="#marginal-densities-in-the-continuous-setting" id="toc-marginal-densities-in-the-continuous-setting" class="nav-link" data-scroll-target="#marginal-densities-in-the-continuous-setting"><span class="header-section-number">6.6.8</span> Marginal Densities in the Continuous Setting</a></li>
  <li><a href="#poker-interpretation" id="toc-poker-interpretation" class="nav-link" data-scroll-target="#poker-interpretation"><span class="header-section-number">6.6.9</span> Poker Interpretation</a></li>
  <li><a href="#marginalization-as-information-loss" id="toc-marginalization-as-information-loss" class="nav-link" data-scroll-target="#marginalization-as-information-loss"><span class="header-section-number">6.6.10</span> Marginalization as Information Loss</a></li>
  <li><a href="#marginal-distributions" id="toc-marginal-distributions" class="nav-link" data-scroll-target="#marginal-distributions"><span class="header-section-number">6.6.11</span> Marginal Distributions</a></li>
  <li><a href="#interpretation-of-the-parameter-lambda" id="toc-interpretation-of-the-parameter-lambda" class="nav-link" data-scroll-target="#interpretation-of-the-parameter-lambda"><span class="header-section-number">6.6.12</span> Interpretation of the Parameter <span class="math inline">\lambda</span></a></li>
  <li><a href="#poker-interpretation-1" id="toc-poker-interpretation-1" class="nav-link" data-scroll-target="#poker-interpretation-1"><span class="header-section-number">6.6.13</span> Poker Interpretation</a></li>
  <li><a href="#visualizing-the-joint-density-of-equity-and-bet-size" id="toc-visualizing-the-joint-density-of-equity-and-bet-size" class="nav-link" data-scroll-target="#visualizing-the-joint-density-of-equity-and-bet-size"><span class="header-section-number">6.6.14</span> Visualizing the Joint Density of Equity and Bet Size</a></li>
  <li><a href="#bayes-theorem-in-the-continuous-setting" id="toc-bayes-theorem-in-the-continuous-setting" class="nav-link" data-scroll-target="#bayes-theorem-in-the-continuous-setting"><span class="header-section-number">6.6.15</span> Bayes’ Theorem in the Continuous Setting</a></li>
  <li><a href="#theorem-bayes-theorem-continuous-version" id="toc-theorem-bayes-theorem-continuous-version" class="nav-link" data-scroll-target="#theorem-bayes-theorem-continuous-version"><span class="header-section-number">6.6.16</span> Theorem (Bayes’ Theorem, Continuous Version)</a></li>
  <li><a href="#the-propto-notation" id="toc-the-propto-notation" class="nav-link" data-scroll-target="#the-propto-notation"><span class="header-section-number">6.6.17</span> The <span class="math inline">\propto</span> Notation</a></li>
  <li><a href="#why-this-notation-is-useful" id="toc-why-this-notation-is-useful" class="nav-link" data-scroll-target="#why-this-notation-is-useful"><span class="header-section-number">6.6.18</span> Why This Notation Is Useful</a></li>
  <li><a href="#example-bayes-theorem-for-equity-given-a-bet" id="toc-example-bayes-theorem-for-equity-given-a-bet" class="nav-link" data-scroll-target="#example-bayes-theorem-for-equity-given-a-bet"><span class="header-section-number">6.6.19</span> Example: Bayes’ Theorem for Equity Given a Bet</a></li>
  <li><a href="#identifying-the-likelihood" id="toc-identifying-the-likelihood" class="nav-link" data-scroll-target="#identifying-the-likelihood"><span class="header-section-number">6.6.20</span> Identifying the Likelihood</a></li>
  <li><a href="#applying-bayes-theorem" id="toc-applying-bayes-theorem" class="nav-link" data-scroll-target="#applying-bayes-theorem"><span class="header-section-number">6.6.21</span> Applying Bayes’ Theorem</a></li>
  <li><a href="#interpretation" id="toc-interpretation" class="nav-link" data-scroll-target="#interpretation"><span class="header-section-number">6.6.22</span> Interpretation</a></li>
  <li><a href="#visualizing-prior-vs-posterior-densities" id="toc-visualizing-prior-vs-posterior-densities" class="nav-link" data-scroll-target="#visualizing-prior-vs-posterior-densities"><span class="header-section-number">6.6.23</span> Visualizing Prior vs Posterior Densities</a></li>
  <li><a href="#conceptual-summary" id="toc-conceptual-summary" class="nav-link" data-scroll-target="#conceptual-summary"><span class="header-section-number">6.6.24</span> Conceptual Summary</a></li>
  <li><a href="#conceptual-summary-1" id="toc-conceptual-summary-1" class="nav-link" data-scroll-target="#conceptual-summary-1"><span class="header-section-number">6.6.25</span> Conceptual Summary</a></li>
  <li><a href="#bayes-theorem-with-continuous-likelihoods" id="toc-bayes-theorem-with-continuous-likelihoods" class="nav-link" data-scroll-target="#bayes-theorem-with-continuous-likelihoods"><span class="header-section-number">6.6.26</span> Bayes’ Theorem with Continuous Likelihoods</a></li>
  <li><a href="#synthesis-sums-integrals-and-information" id="toc-synthesis-sums-integrals-and-information" class="nav-link" data-scroll-target="#synthesis-sums-integrals-and-information"><span class="header-section-number">6.6.27</span> Synthesis: Sums, Integrals, and Information</a></li>
  </ul></li>
  <li><a href="#beyond-poker-bayesian-updating-in-machine-learning" id="toc-beyond-poker-bayesian-updating-in-machine-learning" class="nav-link" data-scroll-target="#beyond-poker-bayesian-updating-in-machine-learning"><span class="header-section-number">6.7</span> Beyond Poker: Bayesian Updating in Machine Learning</a>
  <ul class="collapse">
  <li><a href="#models-as-beliefs-data-as-evidence" id="toc-models-as-beliefs-data-as-evidence" class="nav-link" data-scroll-target="#models-as-beliefs-data-as-evidence"><span class="header-section-number">6.7.1</span> Models as Beliefs, Data as Evidence</a></li>
  <li><a href="#a-concrete-example-learning-to-recognize-handwritten-digits" id="toc-a-concrete-example-learning-to-recognize-handwritten-digits" class="nav-link" data-scroll-target="#a-concrete-example-learning-to-recognize-handwritten-digits"><span class="header-section-number">6.7.2</span> A Concrete Example: Learning to Recognize Handwritten Digits</a></li>
  <li><a href="#sequential-learning-from-one-data-point-to-many" id="toc-sequential-learning-from-one-data-point-to-many" class="nav-link" data-scroll-target="#sequential-learning-from-one-data-point-to-many"><span class="header-section-number">6.7.3</span> Sequential Learning: From One Data Point to Many</a></li>
  <li><a href="#continuous-parameters-and-integrals-in-practice" id="toc-continuous-parameters-and-integrals-in-practice" class="nav-link" data-scroll-target="#continuous-parameters-and-integrals-in-practice"><span class="header-section-number">6.7.4</span> Continuous Parameters and Integrals in Practice</a></li>
  <li><a href="#prediction-uncertainty-and-decision-making" id="toc-prediction-uncertainty-and-decision-making" class="nav-link" data-scroll-target="#prediction-uncertainty-and-decision-making"><span class="header-section-number">6.7.5</span> Prediction, Uncertainty, and Decision-Making</a></li>
  <li><a href="#what-poker-teaches-machine-learningand-vice-versa" id="toc-what-poker-teaches-machine-learningand-vice-versa" class="nav-link" data-scroll-target="#what-poker-teaches-machine-learningand-vice-versa"><span class="header-section-number">6.7.6</span> What Poker Teaches Machine Learning—and Vice Versa</a></li>
  </ul></li>
  <li><a href="#liberal-arts-sidebar-conditional-probability-and-the-justice-system" id="toc-liberal-arts-sidebar-conditional-probability-and-the-justice-system" class="nav-link" data-scroll-target="#liberal-arts-sidebar-conditional-probability-and-the-justice-system"><span class="header-section-number">6.8</span> Liberal Arts Sidebar: Conditional Probability and the Justice System</a>
  <ul class="collapse">
  <li><a href="#evidence-is-not-guilt" id="toc-evidence-is-not-guilt" class="nav-link" data-scroll-target="#evidence-is-not-guilt"><span class="header-section-number">6.8.1</span> Evidence Is Not Guilt</a></li>
  <li><a href="#base-rates-and-the-prosecutors-fallacy" id="toc-base-rates-and-the-prosecutors-fallacy" class="nav-link" data-scroll-target="#base-rates-and-the-prosecutors-fallacy"><span class="header-section-number">6.8.2</span> Base Rates and the Prosecutor’s Fallacy</a></li>
  <li><a href="#risk-assessment-and-conditional-predictions" id="toc-risk-assessment-and-conditional-predictions" class="nav-link" data-scroll-target="#risk-assessment-and-conditional-predictions"><span class="header-section-number">6.8.3</span> Risk Assessment and Conditional Predictions</a></li>
  <li><a href="#individual-cases-and-group-statistics" id="toc-individual-cases-and-group-statistics" class="nav-link" data-scroll-target="#individual-cases-and-group-statistics"><span class="header-section-number">6.8.4</span> Individual Cases and Group Statistics</a></li>
  <li><a href="#transparency-interpretation-and-trust" id="toc-transparency-interpretation-and-trust" class="nav-link" data-scroll-target="#transparency-interpretation-and-trust"><span class="header-section-number">6.8.5</span> Transparency, Interpretation, and Trust</a></li>
  <li><a href="#a-shared-structure" id="toc-a-shared-structure" class="nav-link" data-scroll-target="#a-shared-structure"><span class="header-section-number">6.8.6</span> A Shared Structure</a></li>
  </ul></li>
  <li><a href="#homework-problems" id="toc-homework-problems" class="nav-link" data-scroll-target="#homework-problems"><span class="header-section-number">6.9</span> Homework Problems</a>
  <ul class="collapse">
  <li><a href="#problems-111-discrete-conditional-probability-and-bayes-theorem" id="toc-problems-111-discrete-conditional-probability-and-bayes-theorem" class="nav-link" data-scroll-target="#problems-111-discrete-conditional-probability-and-bayes-theorem"><span class="header-section-number">6.9.1</span> Problems 1–11: Discrete Conditional Probability and Bayes’ Theorem</a></li>
  <li><a href="#problems-1217-continuous-conditional-probability-and-bayes-theorem" id="toc-problems-1217-continuous-conditional-probability-and-bayes-theorem" class="nav-link" data-scroll-target="#problems-1217-continuous-conditional-probability-and-bayes-theorem"><span class="header-section-number">6.9.2</span> Problems 12–17: Continuous Conditional Probability and Bayes’ Theorem</a></li>
  </ul></li>
  <li><a href="#extension-activity" id="toc-extension-activity" class="nav-link" data-scroll-target="#extension-activity"><span class="header-section-number">6.10</span> Extension Activity</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Conditional Probability and Bayes’ Rule</span></h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p><strong>Chapter at a glance</strong></p>
<ul>
<li><strong>Part:</strong> Part II: Probability, Expectation, and Belief Updating</li>
<li><strong>Poker topics:</strong> 9 Bayesian Updating (Implicit), 10 Information Asymmetry</li>
<li><strong>Beyond poker:</strong> medical testing (base rates), legal reasoning (evidence evaluation), machine learning (classification/update)</li>
</ul>
</div>
</div>
<section id="motivating-example-what-does-a-bet-tell-you" class="level2" data-number="6.1">
<h2 data-number="6.1" class="anchored" data-anchor-id="motivating-example-what-does-a-bet-tell-you"><span class="header-section-number">6.1</span> Motivating Example: What Does a Bet Tell You?</h2>
<p>Late in a no-limit Texas Hold’em tournament, two players see a flop. No cards are revealed, but one player makes a large bet—much larger than the pot itself. The other player must now decide whether to fold, call, or raise.</p>
<p>Nothing about the physical state of the game has changed. The cards are exactly what they were a moment ago. And yet, the situation feels fundamentally different.</p>
<p>Before the bet, many hands seemed plausible. After the bet, some hands feel far more likely than others.</p>
<p>This shift—from <em>what was possible</em> to <em>what now seems plausible</em>—is the phenomenon we want to understand mathematically.</p>
<hr>
<section id="the-puzzle" class="level3" data-number="6.1.1">
<h3 data-number="6.1.1" class="anchored" data-anchor-id="the-puzzle"><span class="header-section-number">6.1.1</span> The Puzzle</h3>
<p>Suppose that, before the flop action, the betting player could plausibly be holding any one of several hand types: - very strong hands, - medium-strength hands, - weak or speculative hands.</p>
<p>We may not know which is correct, but we may have reasonable beliefs about their relative likelihoods.</p>
<p>Now the bet arrives.</p>
<p>Intuitively, a very large bet seems more consistent with some hands than with others. Strong hands might make such a bet frequently; weak hands might do so rarely. Medium hands might fall somewhere in between.</p>
<p>The central question is not <em>what hand the player actually has</em>.<br>
The question is:</p>
<blockquote class="blockquote">
<p><strong>How should our beliefs about the possible hands change once the bet is observed?</strong></p>
</blockquote>
<hr>
</section>
<section id="why-naive-reasoning-fails" class="level3" data-number="6.1.2">
<h3 data-number="6.1.2" class="anchored" data-anchor-id="why-naive-reasoning-fails"><span class="header-section-number">6.1.2</span> Why Naive Reasoning Fails</h3>
<p>It is tempting to reason informally:</p>
<ul>
<li>“Large bets usually mean strong hands.”</li>
<li>“Sometimes players bluff.”</li>
<li>“This player has bluffed before.”</li>
</ul>
<p>All of these statements may be true—but none of them specify <em>how much</em> belief should shift, or <em>how</em> multiple pieces of information should be combined.</p>
<p>If we are not careful, we risk two common mistakes: 1. <strong>Overreacting</strong> to new information, treating one action as decisive. 2. <strong>Underreacting</strong>, failing to revise beliefs even when evidence is strong.</p>
<p>We need a systematic rule that tells us: - how to start with an initial belief, - how to incorporate observed actions, - and how to arrive at an updated belief that is mathematically consistent.</p>
<hr>
</section>
<section id="hidden-states-and-observable-evidence" class="level3" data-number="6.1.3">
<h3 data-number="6.1.3" class="anchored" data-anchor-id="hidden-states-and-observable-evidence"><span class="header-section-number">6.1.3</span> Hidden States and Observable Evidence</h3>
<p>This hand already contains the essential structure of a probabilistic inference problem.</p>
<ul>
<li>The opponent’s <strong>cards</strong> are hidden.</li>
<li>The opponent’s <strong>bet</strong> is observable.</li>
<li>The bet depends (perhaps imperfectly) on the cards.</li>
<li>We must reason backward from the observed action to the hidden cause.</li>
</ul>
<p>Poker forces us to confront this structure repeatedly. We never see the full state of the world; we see only fragments of evidence, arriving in sequence.</p>
<p>The mathematics that governs this reasoning is not specific to poker. It appears whenever we reason from effects to causes, from data to explanation, or from observation to belief.</p>
<hr>
</section>
<section id="the-need-for-conditional-probability" class="level3" data-number="6.1.4">
<h3 data-number="6.1.4" class="anchored" data-anchor-id="the-need-for-conditional-probability"><span class="header-section-number">6.1.4</span> The Need for Conditional Probability</h3>
<p>To answer the question raised by this hand, we must be able to say things like:</p>
<ul>
<li>What is the probability of seeing a bet <em>given</em> a strong hand?</li>
<li>What is the probability of a strong hand <em>given</em> that a bet was observed?</li>
<li>How do these two quantities differ?</li>
<li>How should prior beliefs be adjusted in light of new evidence?</li>
</ul>
<p>These questions are not interchangeable. Confusing them leads to faulty inference.</p>
<p>Conditional probability provides the language to distinguish them. Bayes’ theorem provides the rule that connects them.</p>
<p>The remainder of this chapter develops that mathematics, beginning with simple discrete models and gradually moving to more realistic continuous ones. Poker will remain our guide—not as a strategy manual, but as a laboratory for understanding how rational belief revision works under uncertainty.</p>
</section>
</section>
<section id="mathematical-framework-and-poker-theory" class="level2" data-number="6.2">
<h2 data-number="6.2" class="anchored" data-anchor-id="mathematical-framework-and-poker-theory"><span class="header-section-number">6.2</span> Mathematical Framework and Poker Theory</h2>
<p>This chapter studies how probabilities change when new information is observed. The mathematics underlying this process is conditional probability and Bayes’ theorem. Poker serves as a concrete setting in which uncertainty is genuine, information arrives sequentially, and hidden states are never directly revealed.</p>
<p>At the start of a poker hand, players have only partial information: their own cards, the betting structure, and prior experience. As the hand unfolds, additional information arrives through community cards and observed actions. What changes is not the underlying deal of the cards, but the <em>beliefs</em> players hold about one another. Conditional probability is the mathematical language for tracking these belief changes.</p>
<p>We begin in the discrete setting, where the logical structure of belief updating is most transparent. We then revisit the same ideas in continuous probability spaces, where observable evidence—such as bet sizes—varies along a continuum.</p>
<p>Throughout the chapter, we will return to two running examples: 1. a <strong>discrete holding-category model</strong>, and<br>
2. a <strong>continuous bet-size model</strong>.</p>
<hr>
</section>
<section id="discrete-probability" class="level2" data-number="6.3">
<h2 data-number="6.3" class="anchored" data-anchor-id="discrete-probability"><span class="header-section-number">6.3</span> Discrete Probability</h2>
<section id="conditional-joint-and-total-probability" class="level3" data-number="6.3.1">
<h3 data-number="6.3.1" class="anchored" data-anchor-id="conditional-joint-and-total-probability"><span class="header-section-number">6.3.1</span> Conditional, Joint, and Total Probability</h3>
<p>We work first with a finite or countable probability space <span class="math inline">(\Omega,\mathcal{F},\mathbb{P})</span>. In this setting, probabilities can be assigned directly to individual outcomes or categories, making the mechanics of conditioning easy to see.</p>
<p>The basic objects of interest are <em>joint</em>, <em>conditional</em>, and <em>marginal</em> probabilities. Each corresponds to a different informational viewpoint: joint probability describes what is true simultaneously, conditional probability describes what is true given information, and marginal probability describes what is true when some information is ignored.</p>
<p><strong>Definition (Joint Probability).</strong> For events <span class="math inline">A,B \subseteq \Omega</span>, the joint probability of <span class="math inline">A</span> and <span class="math inline">B</span> is <span class="math display">                                   
\mathbb{P}(A \cap B).                                                                    </span><br>
Joint probability measures how often two events occur together. It treats both events symmetrically and does not assume that either is known first.</p>
<p><strong>Definition (Joint Distribution of Discrete Random Variables).</strong> Let <span class="math inline">X_1,\dots,X_n</span> be discrete random variables, each taking values in a finite set. The <strong>joint distribution</strong> of <span class="math inline">(X_1,\dots,X_n)</span> is the collection of probabilities <span class="math display">
\mathbb{P}(X_1=x_1,\; X_2=x_2,\;\dots,\; X_n=x_n),
</span> defined for all possible combinations <span class="math inline">(x_1,\dots,x_n)</span> of values.</p>
<p>The joint distribution encodes all probabilistic information about the variables taken together. Marginal and conditional distributions are obtained from it by summing over appropriate subsets of values.</p>
<hr>
<p><strong>Interpretation.</strong><br>
Joint probability concerns events; joint distributions extend this idea to random variables by assigning probabilities to <em>simultaneous realizations</em> of several variables.</p>
<hr>
<p><em>Running Example 1 (Discrete setup).</em><br>
Let <span class="math inline">H</span> represent an opponent’s holding category, taking values <span class="math display">
\{\text{strong}, \text{medium}, \text{weak}\}.
</span> Let <span class="math inline">A</span> represent a particular betting action, such as a bet on the flop.</p>
<p>The joint distribution of <span class="math inline">(H,A)</span> assigns probabilities to each holding–action pair.<br>
For example, the joint probability <span class="math display">
\mathbb{P}(H=\text{strong}, A=\text{bet})
</span> represents the frequency with which strong hands and bets occur together.</p>
<p><em>Running Example 1 (Discrete setup, continued).</em></p>
<p>Suppose the opponent’s holding category <span class="math inline">H</span> and betting action <span class="math inline">A</span> have the following joint distribution.</p>
<p>Here, the action <span class="math inline">A</span> takes values in <span class="math inline">\{\text{bet}, \text{check}\}</span>.</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Holding <span class="math inline">H</span></th>
<th><span class="math inline">A=\text{bet}</span></th>
<th><span class="math inline">A=\text{check}</span></th>
<th>Row total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>strong</td>
<td>0.18</td>
<td>0.07</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>medium</td>
<td>0.20</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr class="odd">
<td>weak</td>
<td>0.05</td>
<td>0.20</td>
<td>0.25</td>
</tr>
<tr class="even">
<td><strong>Column total</strong></td>
<td><strong>0.43</strong></td>
<td><strong>0.57</strong></td>
<td><strong>1.00</strong></td>
</tr>
</tbody>
</table>
<p>This table specifies the joint distribution of <span class="math inline">(H,A)</span>.</p>
<ul>
<li>Each row sums to the prior probability of the corresponding holding category.</li>
<li>Each column sum gives the marginal probability of observing that action.</li>
<li>Individual entries represent joint probabilities, such as <span class="math display">
\mathbb{P}(H=\text{strong}, A=\text{bet}) = 0.18.
</span></li>
</ul>
<p>In poker terms, the table encodes how often different hands <em>tend</em> to produce different actions, without assuming that any action uniquely determines a hand.</p>
</section>
<section id="marginal-distributions-from-the-joint-distribution" class="level3" data-number="6.3.2">
<h3 data-number="6.3.2" class="anchored" data-anchor-id="marginal-distributions-from-the-joint-distribution"><span class="header-section-number">6.3.2</span> Marginal Distributions from the Joint Distribution</h3>
<p>The joint distribution of <span class="math inline">(H,A)</span> records probabilities for <em>pairs</em> of outcomes: a holding category and an action. Often, however, we want probabilities for just one variable at a time—either the distribution of holdings <em>ignoring</em> the action, or the distribution of actions <em>ignoring</em> the holding. These single-variable distributions are called <strong>marginal distributions</strong> because they are obtained by “summing out” the other variable.</p>
<hr>
<p><strong>Definition (Marginal Distribution, discrete case).</strong><br>
Let <span class="math inline">(X,Y)</span> be a pair of discrete random variables with joint distribution <span class="math inline">\mathbb{P}(X=x, Y=y)</span>.</p>
<ul>
<li><p>The <strong>marginal distribution of</strong> <span class="math inline">X</span> is the function <span class="math display">
\mathbb{P}(X=x) = \sum_{y} \mathbb{P}(X=x, Y=y),
</span> where the sum is over all possible values of <span class="math inline">Y</span>.</p></li>
<li><p>The <strong>marginal distribution of</strong> <span class="math inline">Y</span> is the function <span class="math display">
\mathbb{P}(Y=y) = \sum_{x} \mathbb{P}(X=x, Y=y),
</span> where the sum is over all possible values of <span class="math inline">X</span>.</p></li>
</ul>
<hr>
</section>
<section id="computing-the-marginals-in-running-example-1" class="level3" data-number="6.3.3">
<h3 data-number="6.3.3" class="anchored" data-anchor-id="computing-the-marginals-in-running-example-1"><span class="header-section-number">6.3.3</span> Computing the Marginals in Running Example 1</h3>
<p>Recall the joint distribution table:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Holding <span class="math inline">H</span></th>
<th><span class="math inline">A=\text{bet}</span></th>
<th><span class="math inline">A=\text{check}</span></th>
<th>Row total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>strong</td>
<td>0.18</td>
<td>0.07</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>medium</td>
<td>0.20</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr class="odd">
<td>weak</td>
<td>0.05</td>
<td>0.20</td>
<td>0.25</td>
</tr>
<tr class="even">
<td><strong>Column total</strong></td>
<td><strong>0.43</strong></td>
<td><strong>0.57</strong></td>
<td><strong>1.00</strong></td>
</tr>
</tbody>
</table>
<hr>
<section id="marginal-distribution-of-h" class="level4" data-number="6.3.3.1">
<h4 data-number="6.3.3.1" class="anchored" data-anchor-id="marginal-distribution-of-h"><span class="header-section-number">6.3.3.1</span> Marginal distribution of <span class="math inline">H</span></h4>
<p>We obtain <span class="math inline">\mathbb{P}(H=h)</span> by summing across actions (row-wise):</p>
<p><span class="math display">
\mathbb{P}(H=\text{strong})
=
\mathbb{P}(H=\text{strong},A=\text{bet})
+
\mathbb{P}(H=\text{strong},A=\text{check})
=
0.18+0.07
=
0.25,
</span></p>
<p><span class="math display">
\mathbb{P}(H=\text{medium})
=
0.20+0.30
=
0.50,
</span></p>
<p><span class="math display">
\mathbb{P}(H=\text{weak})
=
0.05+0.20
=
0.25.
</span></p>
<p>So the marginal distribution of <span class="math inline">H</span> is <span class="math display">
\mathbb{P}(H=\text{strong})=0.25,\quad
\mathbb{P}(H=\text{medium})=0.50,\quad
\mathbb{P}(H=\text{weak})=0.25.
</span></p>
<p>These are the <strong>priors</strong> on holding categories in this model.</p>
<hr>
</section>
<section id="marginal-distribution-of-a" class="level4" data-number="6.3.3.2">
<h4 data-number="6.3.3.2" class="anchored" data-anchor-id="marginal-distribution-of-a"><span class="header-section-number">6.3.3.2</span> Marginal distribution of <span class="math inline">A</span></h4>
<p>We obtain <span class="math inline">\mathbb{P}(A=a)</span> by summing across holdings (column-wise):</p>
<p><span class="math display">
\mathbb{P}(A=\text{bet})
=
0.18+0.20+0.05
=
0.43,
</span></p>
<p><span class="math display">
\mathbb{P}(A=\text{check})
=
0.07+0.30+0.20
=
0.57.
</span></p>
<p>So the marginal distribution of <span class="math inline">A</span> is <span class="math display">
\mathbb{P}(A=\text{bet})=0.43,\quad
\mathbb{P}(A=\text{check})=0.57.
</span></p>
<p>This is the distribution of observed actions <em>before</em> we condition on anything about the opponent’s hand.</p>
<hr>
<p><strong>Poker interpretation.</strong><br>
The marginal distribution of <span class="math inline">H</span> expresses beliefs about the opponent’s holding category <em>before any action is observed</em>. The marginal distribution of <span class="math inline">A</span> describes how often bets and checks occur overall, averaged across all holdings. Bayes’ theorem and conditional probability enter when we ask the inverse question: once we observe <span class="math inline">A</span>, how should the distribution of <span class="math inline">H</span> change?</p>
<p>While marginal distributions are helpful, in poker analysis one is more typically focused on making decisions based on data that is available. In the context of our example for instance, we might ask what is the probability the player has a strong hand, given the infommation that they bet. Condtional Probability gives the framework for this conversation. |</p>
<p><strong>Definition (Conditional Probability).</strong>&nbsp; If <span class="math inline">\mathbb{P}(B)&gt;0</span>, the conditional probability of <span class="math inline">A</span> given <span class="math inline">B</span> is <span class="math display">                                                                                                                                                                                                                                                                                         \mathbb{P}(A \mid B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}.                                                                                                                                                                                                                                                                                                </span></p>
<ul>
<li><p>Conditioning represents an update of information. Once event <span class="math inline">B</span> is known to have occurred, outcomes outside <span class="math inline">B</span> are no longer possible. The remaining probability mass is rescaled so that probabilities still sum to one.<br>
### Conditional Probability and Conditional Distributions in Running Example 1<br>
-Marginal distributions describe what we believe <em>before</em> conditioning on new information. Conditional probability describes how those beliefs change <em>after</em> an event is observed.</p></li>
<li><p>In poker terms, marginal distributions describe beliefs before an action occurs; conditional distributions describe beliefs after we see what the opponent actually does.</p></li>
</ul>
<p><strong>Definition (Conditional Probability, discrete case).</strong><br>
Let <span class="math inline">X</span> and <span class="math inline">Y</span> be discrete random variables with <span class="math inline">\mathbb{P}(Y=y)&gt;0</span>.<br>
The conditional probability of <span class="math inline">X=x</span> given <span class="math inline">Y=y</span> is <span class="math display">
\mathbb{P}(X=x \mid Y=y)
=
\frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}.
</span></p>
<p>Fixing <span class="math inline">Y=y</span> and allowing <span class="math inline">X</span> to vary produces the <strong>conditional distribution of</strong> <span class="math inline">X</span> given <span class="math inline">Y=y</span>.</p>
<hr>
</section>
</section>
<section id="conditioning-on-a-bet-in-running-example-1" class="level3" data-number="6.3.4">
<h3 data-number="6.3.4" class="anchored" data-anchor-id="conditioning-on-a-bet-in-running-example-1"><span class="header-section-number">6.3.4</span> Conditioning on a Bet in Running Example 1</h3>
<p>Recall the joint distribution of holding category <span class="math inline">H</span> and action <span class="math inline">A</span>:</p>
<table class="caption-top table">
<thead>
<tr class="header">
<th>Holding <span class="math inline">H</span></th>
<th><span class="math inline">A=\text{bet}</span></th>
<th><span class="math inline">A=\text{check}</span></th>
<th>Row total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>strong</td>
<td>0.18</td>
<td>0.07</td>
<td>0.25</td>
</tr>
<tr class="even">
<td>medium</td>
<td>0.20</td>
<td>0.30</td>
<td>0.50</td>
</tr>
<tr class="odd">
<td>weak</td>
<td>0.05</td>
<td>0.20</td>
<td>0.25</td>
</tr>
<tr class="even">
<td><strong>Column total</strong></td>
<td><strong>0.43</strong></td>
<td><strong>0.57</strong></td>
<td><strong>1.00</strong></td>
</tr>
</tbody>
</table>
<p>From the column totals, the probability that a bet is observed is <span class="math display">
\mathbb{P}(A=\text{bet}) = 0.43.
</span></p>
<p>We now compute the conditional probabilities of each holding category <em>given that a bet was observed</em>.</p>
<hr>
<section id="conditional-probabilities-given-a-bet" class="level4" data-number="6.3.4.1">
<h4 data-number="6.3.4.1" class="anchored" data-anchor-id="conditional-probabilities-given-a-bet"><span class="header-section-number">6.3.4.1</span> Conditional probabilities given a bet</h4>
<p>Using the definition of conditional probability,</p>
<p><span class="math display">
\mathbb{P}(H=\text{strong} \mid A=\text{bet})
=
\frac{\mathbb{P}(H=\text{strong}, A=\text{bet})}
{\mathbb{P}(A=\text{bet})}
=
\frac{0.18}{0.43}
\approx 0.419,
</span></p>
<p><span class="math display">
\mathbb{P}(H=\text{medium} \mid A=\text{bet})
=
\frac{0.20}{0.43}
\approx 0.465,
</span></p>
<p><span class="math display">
\mathbb{P}(H=\text{weak} \mid A=\text{bet})
=
\frac{0.05}{0.43}
\approx 0.116.
</span></p>
<hr>
</section>
</section>
<section id="the-conditional-distribution-of-h-given-a-bet" class="level3" data-number="6.3.5">
<h3 data-number="6.3.5" class="anchored" data-anchor-id="the-conditional-distribution-of-h-given-a-bet"><span class="header-section-number">6.3.5</span> The Conditional Distribution of <span class="math inline">H</span> Given a Bet</h3>
<p>Collecting these values, the conditional distribution of the holding category given a bet is <span class="math display">
\mathbb{P}(H=\text{strong} \mid A=\text{bet}) \approx 0.42,
\quad
\mathbb{P}(H=\text{medium} \mid A=\text{bet}) \approx 0.47,
\quad
\mathbb{P}(H=\text{weak} \mid A=\text{bet}) \approx 0.11.
</span></p>
<p>This distribution sums to 1, as required.</p>
<hr>
<p><strong>Poker interpretation.</strong><br>
Before any action was observed, the weak hand category had probability <span class="math inline">0.25</span>. After observing a bet, that probability drops to about <span class="math inline">0.11</span>. Strong hands increase in relative plausibility, even though they are still not certain.</p>
<p>This illustrates the core idea of conditional probability:<br>
<strong>observing an action reshapes belief without eliminating uncertainty</strong>.</p>
</section>
</section>
<section id="the-opponents-hand-has-not-changed.-only-our-information-about-it-has." class="level2" data-number="6.4">
<h2 data-number="6.4" class="anchored" data-anchor-id="the-opponents-hand-has-not-changed.-only-our-information-about-it-has."><span class="header-section-number">6.4</span> The opponent’s hand has not changed. Only our information about it has.</h2>
<p><strong>Definition (Law of Total Probability).</strong><br>
Let <span class="math inline">\{B_1,\dots,B_n\}</span> be a partition of <span class="math inline">\Omega</span> with <span class="math inline">\mathbb{P}(B_i)&gt;0</span>. Then for any event <span class="math inline">A</span>, <span class="math display">
\mathbb{P}(A) = \sum_{i=1}^n \mathbb{P}(A \mid B_i)\mathbb{P}(B_i).
</span></p>
<p>This identity expresses the probability of <span class="math inline">A</span> as a weighted average across mutually exclusive scenarios.</p>
<hr>
<section id="bayes-theorem-discrete" class="level3" data-number="6.4.1">
<h3 data-number="6.4.1" class="anchored" data-anchor-id="bayes-theorem-discrete"><span class="header-section-number">6.4.1</span> Bayes’ Theorem (Discrete)</h3>
<p>The definitions above describe how probabilities are related when information flows forward. Bayes’ theorem addresses the inverse problem: how beliefs about hidden causes should change when an effect is observed.</p>
<hr>
<p><strong>Theorem (Bayes’ Theorem, discrete).</strong><br>
Let <span class="math inline">A,B \in \mathcal{F}</span> with <span class="math inline">\mathbb{P}(A)&gt;0</span> and <span class="math inline">\mathbb{P}(B)&gt;0</span>. Then <span class="math display">
\mathbb{P}(A \mid B)
=
\frac{\mathbb{P}(B \mid A)\mathbb{P}(A)}{\mathbb{P}(B)}.
</span></p>
<table class="caption-top table">
<colgroup>
<col style="width: 100%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">### Bayes’ Theorem in Running Example 1</td>
</tr>
</tbody>
</table>
<p><span class="math display">                       
\mathbb{P}(H=h \mid A=\text{bet}) =                                                        
\frac{\mathbb{P}(H=h, A=\text{bet})}{\mathbb{P}(A=\text{bet})}.                          </span><br>
Bayes’ theorem gives the same quantities, but it does so in a way that separates the update into two conceptual pieces: - a <strong>prior</strong> belief about the hidden state <span class="math inline">H</span>, and - a <strong>likelihood</strong> model describing how actions arise from each hidden state. This is the form of the calculation we will reuse throughout the chapter. |</p>
<p><strong>Bayes’ Theorem (random-variable form, discrete).</strong><br>
If <span class="math inline">\mathbb{P}(A=\text{bet})&gt;0</span>, then <span class="math display">
\mathbb{P}(H=h \mid A=\text{bet})
=
\frac{\mathbb{P}(A=\text{bet} \mid H=h)\,\mathbb{P}(H=h)}
{\mathbb{P}(A=\text{bet})}.
</span></p>
<p>The denominator can be computed from the law of total probability: <span class="math display">
\mathbb{P}(A=\text{bet})
=
\sum_{h'} \mathbb{P}(A=\text{bet} \mid H=h')\,\mathbb{P}(H=h').
</span></p>
<hr>
</section>
<section id="step-1-extract-priors-and-likelihoods-from-the-table" class="level3" data-number="6.4.2">
<h3 data-number="6.4.2" class="anchored" data-anchor-id="step-1-extract-priors-and-likelihoods-from-the-table"><span class="header-section-number">6.4.2</span> Step 1: Extract priors and likelihoods from the table</h3>
<p>From the joint distribution table, the prior distribution of <span class="math inline">H</span> (row totals) is: <span class="math display">
\mathbb{P}(H=\text{strong})=0.25,\quad
\mathbb{P}(H=\text{medium})=0.50,\quad
\mathbb{P}(H=\text{weak})=0.25.
</span></p>
<p>Next compute the likelihoods <span class="math inline">\mathbb{P}(A=\text{bet}\mid H=h)</span> by conditioning within each row:</p>
<p><span class="math display">
\mathbb{P}(A=\text{bet}\mid H=\text{strong})
=
\frac{0.18}{0.25}
=
0.72,
</span></p>
<p><span class="math display">
\mathbb{P}(A=\text{bet}\mid H=\text{medium})
=
\frac{0.20}{0.50}
=
0.40,
</span></p>
<p><span class="math display">
\mathbb{P}(A=\text{bet}\mid H=\text{weak})
=
\frac{0.05}{0.25}
=
0.20.
</span></p>
<hr>
</section>
<section id="step-2-compute-the-evidence-term-mathbbpatextbet" class="level3" data-number="6.4.3">
<h3 data-number="6.4.3" class="anchored" data-anchor-id="step-2-compute-the-evidence-term-mathbbpatextbet"><span class="header-section-number">6.4.3</span> Step 2: Compute the evidence term <span class="math inline">\mathbb{P}(A=\text{bet})</span></h3>
<p>Using the law of total probability, <span class="math display">
\mathbb{P}(A=\text{bet})
=
(0.72)(0.25) + (0.40)(0.50) + (0.20)(0.25)
=
0.18 + 0.20 + 0.05
=
0.43.
</span></p>
<p>This matches the column total in the joint table, as it must.</p>
<hr>
</section>
<section id="step-3-compute-the-posterior-distribution" class="level3" data-number="6.4.4">
<h3 data-number="6.4.4" class="anchored" data-anchor-id="step-3-compute-the-posterior-distribution"><span class="header-section-number">6.4.4</span> Step 3: Compute the posterior distribution</h3>
<p>Now apply Bayes’ theorem to each holding category.</p>
<p>For a strong hand: <span class="math display">
\mathbb{P}(H=\text{strong}\mid A=\text{bet})
=
\frac{(0.72)(0.25)}{0.43}
=
\frac{0.18}{0.43}
\approx 0.419.
</span></p>
<p>For a medium hand: <span class="math display">
\mathbb{P}(H=\text{medium}\mid A=\text{bet})
=
\frac{(0.40)(0.50)}{0.43}
=
\frac{0.20}{0.43}
\approx 0.465.
</span></p>
<p>For a weak hand: <span class="math display">
\mathbb{P}(H=\text{weak}\mid A=\text{bet})
=
\frac{(0.20)(0.25)}{0.43}
=
\frac{0.05}{0.43}
\approx 0.116.
</span></p>
<p>Thus the posterior distribution is <span class="math display">
\mathbb{P}(H=\text{strong} \mid A=\text{bet}) \approx 0.42,\quad
\mathbb{P}(H=\text{medium} \mid A=\text{bet}) \approx 0.47,\quad
\mathbb{P}(H=\text{weak} \mid A=\text{bet}) \approx 0.11.
</span></p>
<hr>
<p><strong>Poker interpretation.</strong><br>
Bayes’ theorem makes the “belief update” visibly modular:</p>
<ul>
<li>the <strong>prior</strong> reflects what was plausible before the action,</li>
<li>the <strong>likelihood</strong> measures how compatible the observed action is with each hidden state,</li>
<li>the <strong>posterior</strong> reweights the hidden states accordingly.</li>
</ul>
<p>A bet does not certify a strong hand, but it changes the balance of plausibility in a mathematically controlled way.</p>
<hr>
</section>
<section id="hidden-versus-observable-random-variables-discrete" class="level3" data-number="6.4.5">
<h3 data-number="6.4.5" class="anchored" data-anchor-id="hidden-versus-observable-random-variables-discrete"><span class="header-section-number">6.4.5</span> Hidden Versus Observable Random Variables (Discrete)</h3>
<p>To apply Bayes’ theorem systematically, we distinguish between <em>hidden</em> and <em>observable</em> random variables.</p>
<p>Let: - <span class="math inline">H</span> be a discrete random variable representing a hidden state (holding category), - <span class="math inline">A</span> be a discrete random variable representing an observable action.</p>
<p>The joint distribution <span class="math inline">\mathbb{P}(H=h, A=a)</span> encodes how often each hidden–observable pair occurs.</p>
<p>From this joint distribution we obtain: - the <strong>prior</strong> <span class="math inline">\mathbb{P}(H=h)</span>, - the <strong>likelihood</strong> <span class="math inline">\mathbb{P}(A=a \mid H=h)</span>, - the <strong>posterior</strong> <span class="math inline">\mathbb{P}(H=h \mid A=a)</span>.</p>
<hr>
</section>
<section id="transition-why-move-to-continuous-models" class="level3" data-number="6.4.6">
<h3 data-number="6.4.6" class="anchored" data-anchor-id="transition-why-move-to-continuous-models"><span class="header-section-number">6.4.6</span> Transition: Why Move to Continuous Models?</h3>
<p>Discrete models clarify logic, but many observables in poker—such as bet size—are not naturally discrete. The <em>magnitude</em> of a bet carries information beyond the fact that a bet occurred.</p>
<p>To capture this information faithfully, we move to continuous probability models.</p>
<hr>
</section>
</section>
<section id="continuous-probability" class="level2" data-number="6.5">
<h2 data-number="6.5" class="anchored" data-anchor-id="continuous-probability"><span class="header-section-number">6.5</span> Continuous Probability</h2>
</section>
<section id="from-discrete-to-continuous-equity-and-bet-size" class="level2" data-number="6.6">
<h2 data-number="6.6" class="anchored" data-anchor-id="from-discrete-to-continuous-equity-and-bet-size"><span class="header-section-number">6.6</span> From Discrete to Continuous: Equity and Bet Size</h2>
<p>In the discrete models above, hidden states were represented by coarse categories such as <em>strong</em>, <em>medium</em>, and <em>weak</em>. While useful for clarifying structure, such categories compress substantial information. In practice, hands vary continuously in strength, and actions vary continuously in magnitude.</p>
<p>To model this finer structure, we introduce <strong>hand equity</strong> and <strong>bet size</strong> as continuous random variables.</p>
<hr>
<section id="definition-equity-of-a-hand" class="level3" data-number="6.6.1">
<h3 data-number="6.6.1" class="anchored" data-anchor-id="definition-equity-of-a-hand"><span class="header-section-number">6.6.1</span> Definition: Equity of a Hand</h3>
<p><strong>Definition (Hand Equity).</strong><br>
The <strong>equity</strong> of a poker hand is the probability that the hand will win (or tie, with ties appropriately weighted) against an opponent’s range, conditional on all currently available information.</p>
<p>Formally, equity is a number in the interval <span class="math inline">[0,1]</span>.</p>
<ul>
<li>An equity of <span class="math inline">0</span> corresponds to a hand that cannot win.</li>
<li>An equity of <span class="math inline">1</span> corresponds to a hand that is guaranteed to win.</li>
<li>Intermediate values represent probabilistic strength.</li>
</ul>
<p>Equity is not a realized outcome; it is a probability. It summarizes uncertainty about future cards and hidden opponent holdings into a single quantitative measure.</p>
<hr>
<p><strong>Interpretation.</strong><br>
Equity plays the role of a <em>continuous hidden state</em>. Unlike the discrete categories used earlier, equity varies smoothly and reflects gradual differences in hand strength.</p>
<hr>
</section>
<section id="bet-size-as-a-continuous-observable" class="level3" data-number="6.6.2">
<h3 data-number="6.6.2" class="anchored" data-anchor-id="bet-size-as-a-continuous-observable"><span class="header-section-number">6.6.2</span> Bet Size as a Continuous Observable</h3>
<p>To model betting behavior, we normalize bet sizes relative to the pot.</p>
<p><strong>Definition (Normalized Bet Size).</strong><br>
Let <span class="math inline">B</span> denote the size of a bet as a fraction of the current pot. We assume <span class="math display">
B \in [0,1],
</span> where: - <span class="math inline">B=0</span> corresponds to checking, - values close to <span class="math inline">1</span> correspond to bets approaching the size of the pot.</p>
<p>This normalization removes dependence on absolute chip counts and isolates the informational content of the bet.</p>
<hr>
</section>
<section id="joint-distribution-of-equity-and-bet-size" class="level3" data-number="6.6.3">
<h3 data-number="6.6.3" class="anchored" data-anchor-id="joint-distribution-of-equity-and-bet-size"><span class="header-section-number">6.6.3</span> Joint Distribution of Equity and Bet Size</h3>
<p>We now model the relationship between hand strength and betting behavior using a joint distribution.</p>
<p>Let: - <span class="math inline">E</span> be a continuous random variable representing hand equity, taking values in <span class="math inline">[0,1]</span>, - <span class="math inline">B</span> be a continuous random variable representing normalized bet size, taking values in <span class="math inline">[0,1)</span>.</p>
<hr>
<p><strong>Definition (Joint Distribution of Equity and Bet Size).</strong><br>
A <strong>joint distribution</strong> of <span class="math inline">(E,B)</span> is specified by a joint probability density function <span class="math display">
f_{E,B}(e,b),
</span> defined for <span class="math inline">e \in [0,1]</span> and <span class="math inline">b \in [0,1]</span>, such that <span class="math display">
\int_0^1 \int_0^1 f_{E,B}(e,b)\, db\, de = 1.
</span></p>
<p>For any regions <span class="math inline">A \subseteq [0,1]</span> and <span class="math inline">C \subseteq [0,1]</span>, <span class="math display">
\mathbb{P}(E \in A,\; B \in C)
=
\int_A \int_C f_{E,B}(e,b)\, db\, de.
</span></p>
<hr>
<p><strong>Interpretation.</strong><br>
The joint density <span class="math inline">f_{E,B}(e,b)</span> encodes how likely different combinations of equity and bet size are to occur together. It is the continuous analogue of the joint probability table used in Running Example 1.</p>
</section>
<section id="a-simple-joint-density-for-equity-and-bet-size" class="level3" data-number="6.6.4">
<h3 data-number="6.6.4" class="anchored" data-anchor-id="a-simple-joint-density-for-equity-and-bet-size"><span class="header-section-number">6.6.4</span> A Simple Joint Density for Equity and Bet Size</h3>
<p>To work concretely in the continuous setting, we now introduce a simple joint density for hand equity and bet size that allows us to model <em>dependence</em> while keeping calculations transparent.</p>
<p>The purpose of this model is not realism in every detail, but clarity: it lets us see exactly how dependence between a hidden variable and an observable signal affects belief updating.</p>
<hr>
</section>
<section id="modeling-setup" class="level3" data-number="6.6.5">
<h3 data-number="6.6.5" class="anchored" data-anchor-id="modeling-setup"><span class="header-section-number">6.6.5</span> Modeling Setup</h3>
<p>Let: - <span class="math inline">E \in [0,1]</span> denote the equity of a hand, - <span class="math inline">B \in [0,1)</span> denote the size of a bet as a fraction of the pot.</p>
<p>We treat both as continuous random variables supported on the unit interval.</p>
<hr>
</section>
<section id="definition-a-parametric-joint-density" class="level3" data-number="6.6.6">
<h3 data-number="6.6.6" class="anchored" data-anchor-id="definition-a-parametric-joint-density"><span class="header-section-number">6.6.6</span> Definition: A Parametric Joint Density</h3>
<p>For a parameter <span class="math inline">\lambda \in [-1,1]</span>, define the joint density <span class="math display">
f_{E,B}(e,b)
=
1 + \lambda(2e - 1)(2b - 1),
\qquad 0 \le e \le 1,\; 0 \le b &lt; 1.
</span></p>
<p>Outside this region, the density is zero.</p>
<hr>
</section>
<section id="why-this-is-a-valid-density" class="level3" data-number="6.6.7">
<h3 data-number="6.6.7" class="anchored" data-anchor-id="why-this-is-a-valid-density"><span class="header-section-number">6.6.7</span> Why This Is a Valid Density</h3>
<p>First, the density integrates to 1: <span class="math display">
\int_0^1 \int_0^1 \bigl[1 + \lambda(2e - 1)(2b - 1)\bigr]\, db\, de = 1.
</span></p>
<p>Second, when <span class="math inline">\lambda \in [-1,1]</span>, the expression <span class="math display">
1 + \lambda(2e - 1)(2b - 1)
</span> is nonnegative for all <span class="math inline">e,b \in [0,1]</span>, ensuring that <span class="math inline">f_{E,B}</span> is a valid probability density.</p>
</section>
<section id="marginal-densities-in-the-continuous-setting" class="level3" data-number="6.6.8">
<h3 data-number="6.6.8" class="anchored" data-anchor-id="marginal-densities-in-the-continuous-setting"><span class="header-section-number">6.6.8</span> Marginal Densities in the Continuous Setting</h3>
<p>In the discrete case, marginal distributions were obtained by summing rows or columns of a joint probability table. In the continuous setting, the same idea applies, but sums are replaced by integrals.</p>
<p>Marginal densities describe what we believe about one variable <em>when we ignore the other</em>. They represent baseline uncertainty before conditioning on new information.</p>
<p><strong>Definition (Marginal Density).</strong><br>
Let <span class="math inline">(X,Y)</span> be continuous random variables with joint density <span class="math inline">f_{X,Y}(x,y)</span>.</p>
<ul>
<li><p>The <strong>marginal density of</strong> <span class="math inline">X</span> is defined by <span class="math display">
f_X(x) = \int f_{X,Y}(x,y)\, dy,
</span> where the integral is taken over all values of <span class="math inline">y</span> in the support of <span class="math inline">Y</span>.</p></li>
<li><p>The <strong>marginal density of</strong> <span class="math inline">Y</span> is defined by <span class="math display">
f_Y(y) = \int f_{X,Y}(x,y)\, dx,
</span> where the integral is taken over all values of <span class="math inline">x</span> in the support of <span class="math inline">X</span>.</p></li>
</ul>
<hr>
<p><strong>Interpretation.</strong><br>
Marginalization removes variables from consideration. It answers questions of the form:</p>
<ul>
<li><em>How likely is each value of</em> <span class="math inline">X</span>, regardless of <span class="math inline">Y</span>?</li>
<li><em>What does our uncertainty about</em> <span class="math inline">Y</span> look like before observing <span class="math inline">X</span>?</li>
</ul>
<p>No conditioning has occurred yet. Marginal densities reflect uncertainty <em>prior</em> to any observation.</p>
<hr>
</section>
<section id="poker-interpretation" class="level3" data-number="6.6.9">
<h3 data-number="6.6.9" class="anchored" data-anchor-id="poker-interpretation"><span class="header-section-number">6.6.9</span> Poker Interpretation</h3>
<p>In our poker model: - <span class="math inline">E</span> represents hand equity, a hidden continuous state. - <span class="math inline">B</span> represents bet size, an observable signal.</p>
<p>The marginal density <span class="math inline">f_E(e)</span> describes how equity is distributed <em>before any betting action is observed</em>.<br>
The marginal density <span class="math inline">f_B(b)</span> describes how often different bet sizes occur <em>on average</em>, across all hands.</p>
<p>Just as in the discrete running example, marginal distributions do not incorporate information from observed actions. They form the baseline against which conditional and posterior distributions are compared.</p>
<hr>
</section>
<section id="marginalization-as-information-loss" class="level3" data-number="6.6.10">
<h3 data-number="6.6.10" class="anchored" data-anchor-id="marginalization-as-information-loss"><span class="header-section-number">6.6.10</span> Marginalization as Information Loss</h3>
<p>It is important to note that marginalization deliberately discards information. When we integrate out a variable, we lose any dependence between variables.</p>
<p>This is not a flaw; it is a feature. Marginal densities provide a reference point. Conditional densities and Bayesian updates describe how this baseline changes when information is introduced.</p>
<p>With this framework in place, we now compute the marginal densities for our specific joint model of equity and bet size.</p>
</section>
<section id="marginal-distributions" class="level3" data-number="6.6.11">
<h3 data-number="6.6.11" class="anchored" data-anchor-id="marginal-distributions"><span class="header-section-number">6.6.11</span> Marginal Distributions</h3>
<p>An important feature of this model is that the marginals are uniform: <span class="math display">
f_E(e) = 1, \qquad f_B(b) = 1.
</span></p>
<p>Thus, <em>before observing any action</em>, all equity values and all bet sizes are equally plausible.</p>
<p>All structure in the model comes from the <strong>dependence</strong> between <span class="math inline">E</span> and <span class="math inline">B</span>, not from the marginals themselves.</p>
<hr>
</section>
<section id="interpretation-of-the-parameter-lambda" class="level3" data-number="6.6.12">
<h3 data-number="6.6.12" class="anchored" data-anchor-id="interpretation-of-the-parameter-lambda"><span class="header-section-number">6.6.12</span> Interpretation of the Parameter <span class="math inline">\lambda</span></h3>
<p>The parameter <span class="math inline">\lambda</span> controls the direction and strength of dependence:</p>
<ul>
<li><p><span class="math inline">\lambda = 0</span><br>
Equity and bet size are independent.</p></li>
<li><p><span class="math inline">\lambda &gt; 0</span><br>
Higher equity values tend to be associated with larger bets, and lower equity values with smaller bets.</p></li>
<li><p><span class="math inline">\lambda &lt; 0</span><br>
Higher equity values tend to be associated with smaller bets, and lower equity values with larger bets.</p></li>
</ul>
<p>The magnitude <span class="math inline">|\lambda|</span> determines how strong this association is.</p>
<hr>
</section>
<section id="poker-interpretation-1" class="level3" data-number="6.6.13">
<h3 data-number="6.6.13" class="anchored" data-anchor-id="poker-interpretation-1"><span class="header-section-number">6.6.13</span> Poker Interpretation</h3>
<p>In this model: - <strong>Equity</strong> is the hidden continuous state, - <strong>Bet size</strong> is the observable signal, - <span class="math inline">\lambda</span> encodes how informative bet size is about equity.</p>
<p>When <span class="math inline">\lambda = 0</span>, observing a bet conveys no information about equity.<br>
When <span class="math inline">\lambda \ne 0</span>, observing a bet reshapes beliefs about equity in a precise, quantifiable way.</p>
<p>This mirrors the discrete running example: the cards do not change, but the <em>distribution</em> describing our beliefs does.</p>
</section>
<section id="visualizing-the-joint-density-of-equity-and-bet-size" class="level3" data-number="6.6.14">
<h3 data-number="6.6.14" class="anchored" data-anchor-id="visualizing-the-joint-density-of-equity-and-bet-size"><span class="header-section-number">6.6.14</span> Visualizing the Joint Density of Equity and Bet Size</h3>
<p>The following figure visualizes the joint density <span class="math display">
f(e,b) = 1 + \lambda(2e-1)(2b-1)
</span> on the unit square.</p>
<div class="cell">
<div class="code-copy-outer-scaffold"><div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Set dependence parameter</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>lambda <span class="ot">&lt;-</span> .<span class="dv">5</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Create grid</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>n <span class="ot">&lt;-</span> <span class="dv">200</span> </span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>e_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> n)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>b_vals <span class="ot">&lt;-</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="at">length.out =</span> n)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>( <span class="at">e =</span> e_vals, <span class="at">b =</span> b_vals )</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Joint density</span></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>grid<span class="sc">$</span>f <span class="ot">&lt;-</span> <span class="dv">1</span> <span class="sc">+</span> lambda <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">*</span> grid<span class="sc">$</span>e <span class="sc">-</span> <span class="dv">1</span>) <span class="sc">*</span> (<span class="dv">2</span> <span class="sc">*</span> grid<span class="sc">$</span>b <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="co"># Plot</span></span>
<span id="cb1-17"><a href="#cb1-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-18"><a href="#cb1-18" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggplot2)</span>
<span id="cb1-19"><a href="#cb1-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-20"><a href="#cb1-20" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(grid, <span class="fu">aes</span>(<span class="at">x =</span> b, <span class="at">y =</span> e, <span class="at">fill =</span> f)) <span class="sc">+</span> <span class="fu">geom_raster</span>() <span class="sc">+</span> <span class="fu">scale_fill_viridis_c</span>(<span class="at">name =</span> <span class="st">"Density"</span>, <span class="at">limit=</span><span class="fu">c</span>(<span class="dv">0</span>,<span class="dv">2</span>)) <span class="sc">+</span> <span class="fu">labs</span>( <span class="at">x =</span> <span class="st">"Bet size (fraction of pot)"</span>, <span class="at">y =</span> <span class="st">"Equity"</span>, <span class="at">title =</span> <span class="fu">expression</span>(<span class="fu">paste</span>(<span class="st">"Joint density f(e,b) = 1 + "</span>, lambda, <span class="st">"(2e-1)(2b-1)"</span>)) ) <span class="sc">+</span> <span class="fu">theme_minimal</span>()</span></code></pre></div><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="05-conditional-probability-and-bayes-rule_files/figure-html/unnamed-chunk-1-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<hr>
<p><strong>Poker interpretation.</strong><br>
In this model, equity summarizes the hidden strength of a hand, while bet size is the observable signal. The joint distribution captures regularities in how hands of different strengths tend to produce bets of different sizes—without assuming that any single bet uniquely identifies a hand.</p>
<p>This sets the stage for continuous versions of conditional probability and Bayes’ theorem.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 100%">
</colgroup>
<tbody>
<tr class="odd">
<td style="text-align: left;">### Conditional Probability in the Continuous Setting</td>
</tr>
<tr class="even">
<td style="text-align: left;">In the continuous setting, probabilities are described by <strong>densities</strong>, not point probabilities. As a result, conditional probability must also be expressed in terms of densities.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">The underlying idea, however, is exactly the same as in the discrete case: conditioning represents belief updating after information is observed.</td>
</tr>
<tr class="even">
<td style="text-align: left;">### Definition: Conditional Density</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><strong>Definition (Conditional Density).</strong> Let <span class="math inline">(X,Y)</span> be continuous random variables with joint density <span class="math inline">f_{X,Y}(x,y)</span> and marginal density <span class="math inline">f_Y(y)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">If <span class="math inline">f_Y(y) &gt; 0</span>, the <strong>conditional density of <span class="math inline">X</span> given <span class="math inline">Y=y</span></strong> is defined by <span class="math display">
f_{X \mid Y}(x \mid y)
=
\frac{f_{X,Y}(x,y)}{f_Y(y)}.
</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">This definition is the continuous analogue of <span class="math display">
\mathbb{P}(X=x \mid Y=y)
=
\frac{\mathbb{P}(X=x, Y=y)}{\mathbb{P}(Y=y)}.
</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">### What Conditioning Means in the Continuous Case</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Because <span class="math inline">\mathbb{P}(Y=y)=0</span> for any exact value <span class="math inline">y</span>, conditioning on <span class="math inline">Y=y</span> does <strong>not</strong> mean that <span class="math inline">Y</span> literally equals <span class="math inline">y</span>. Instead, it should be interpreted as conditioning on an <em>infinitesimally small neighborhood</em> around <span class="math inline">y</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">The conditional density describes how probability mass is distributed <em>relative to</em> the observed value of <span class="math inline">Y</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Fixing <span class="math inline">Y=y</span> and allowing <span class="math inline">X</span> to vary produces a full probability distribution in <span class="math inline">X</span>: <span class="math display">
\int f_{X \mid Y}(x \mid y)\,dx = 1.
</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">### Poker Context: Conditioning on a Bet Size</td>
</tr>
<tr class="odd">
<td style="text-align: left;">We now return to the continuous poker model.</td>
</tr>
<tr class="even">
<td style="text-align: left;">Recall: - <span class="math inline">E \in [0,1]</span> is the <strong>equity</strong> of a hand (hidden), - <span class="math inline">B \in [0,1)</span> is the <strong>bet size</strong> as a fraction of the pot (observed), - the joint density is <span class="math display">
f_{E,B}(e,b) = 1 + \lambda(2e-1)(2b-1),
\qquad 0 \le e,b \le 1,
</span> with <span class="math inline">\lambda \in [-1,1]</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">### Example: Conditional Density of Equity Given a Bet</td>
</tr>
<tr class="even">
<td style="text-align: left;">Suppose we observe a bet of size <span class="math inline">B=b</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">The conditional density of equity given this bet is <span class="math display">
f_{E \mid B}(e \mid b)
=
\frac{f_{E,B}(e,b)}{f_B(b)}.
</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">From the marginal computation (done earlier), we know that <span class="math display">
f_B(b) = 1.
</span></td>
</tr>
<tr class="odd">
<td style="text-align: left;">Therefore, <span class="math display">
f_{E \mid B}(e \mid b)
=
1 + \lambda(2e-1)(2b-1),
\qquad 0 \le e \le 1.
</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">This function is a valid density in <span class="math inline">e</span> for each fixed value of <span class="math inline">b</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">### Interpretation</td>
</tr>
<tr class="even">
<td style="text-align: left;">- If <span class="math inline">\lambda = 0</span>, then <span class="math display">
f_{E \mid B}(e \mid b) = 1,
</span> and observing a bet conveys <strong>no information</strong> about equity.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">- If <span class="math inline">\lambda &gt; 0</span> and <span class="math inline">b</span> is large, then <span class="math inline">(2b-1)&gt;0</span>, and the density places more weight on higher equity values.</td>
</tr>
<tr class="even">
<td style="text-align: left;">- If <span class="math inline">\lambda &gt; 0</span> and <span class="math inline">b</span> is small, then <span class="math inline">(2b-1)&lt;0</span>, and the density favors lower equity values.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">In all cases, the cards have not changed. Only the <em>distribution describing our beliefs</em> has changed. ### Worked Example: ((E&gt;0.75 B=0.75)) from Our Joint Density</td>
</tr>
<tr class="even">
<td style="text-align: left;">Let (E) denote <strong>equity</strong> and (B) denote <strong>bet size in pot units</strong>. In our model, the <strong>joint density</strong> of ((E,B)) on the unit square is</td>
</tr>
<tr class="odd">
<td style="text-align: left;"><span class="math display">
f_{E,B}(e,b) \;=\; 1 + \lambda(2e-1)(2b-1),
\qquad 0\le e\le 1,\; 0\le b\le 1.
</span></td>
</tr>
<tr class="even">
<td style="text-align: left;">(For this to be a valid density everywhere on ([0,1]^2), we need (||), since ((2e-1)(2b-1)).)</td>
</tr>
<tr class="odd">
<td style="text-align: left;">Our goal is to compute</td>
</tr>
<tr class="even">
<td style="text-align: left;"><span class="math display">
\mathbb{P}(E&gt;0.75 \mid B=0.75).
</span></td>
</tr>
</tbody>
</table>
<section id="step-1-compute-the-marginal-density-of-b" class="level4" data-number="6.6.14.1">
<h4 data-number="6.6.14.1" class="anchored" data-anchor-id="step-1-compute-the-marginal-density-of-b"><span class="header-section-number">6.6.14.1</span> Step 1: Compute the marginal density of (B)</h4>
<p>By definition,</p>
<p><span class="math display">
f_B(b) \;=\; \int_0^1 f_{E,B}(e,b)\,de
\;=\; \int_0^1 \left[1+\lambda(2e-1)(2b-1)\right]de.
</span></p>
<p>Split the integral:</p>
<p><span class="math display">
f_B(b)
=
\int_0^1 1\,de
+
\lambda(2b-1)\int_0^1 (2e-1)\,de.
</span></p>
<p>Now,</p>
<p><span class="math display">
\int_0^1 1\,de = 1,
\qquad
\int_0^1 (2e-1)\,de
=
\left[e^2-e\right]_0^1
=
(1-1)-(0-0)=0.
</span></p>
<p>So the marginal is especially simple:</p>
<p><span class="math display">
\boxed{f_B(b)=1 \quad \text{for } 0\le b\le 1.}
</span></p>
<p>That is, (B) is uniformly distributed on ([0,1]) under this joint model.</p>
<hr>
</section>
<section id="step-2-write-down-the-conditional-density-of-e-given-bb" class="level4" data-number="6.6.14.2">
<h4 data-number="6.6.14.2" class="anchored" data-anchor-id="step-2-write-down-the-conditional-density-of-e-given-bb"><span class="header-section-number">6.6.14.2</span> Step 2: Write down the conditional density of (E) given (B=b)</h4>
<p>For a continuous pair ((E,B)), the conditional density is</p>
<p><span class="math display">
f_{E\mid B}(e\mid b) \;=\; \frac{f_{E,B}(e,b)}{f_B(b)}.
</span></p>
<p>Since (f_B(b)=1), we get</p>
<p><span class="math display">
\boxed{
f_{E\mid B}(e\mid b) = 1+\lambda(2e-1)(2b-1),
\quad 0\le e\le 1.
}
</span></p>
<p>Now plug in (b=0.75). Since (2b-1 = 2(0.75)-1 = 0.5),</p>
<p><span class="math display">
f_{E\mid B}(e\mid 0.75)
=
1 + \lambda(2e-1)\cdot 0.5
=
1 + \lambda(e-0.5).
</span></p>
<hr>
</section>
<section id="step-3-integrate-over-e0.75" class="level4" data-number="6.6.14.3">
<h4 data-number="6.6.14.3" class="anchored" data-anchor-id="step-3-integrate-over-e0.75"><span class="header-section-number">6.6.14.3</span> Step 3: Integrate over (e&gt;0.75)</h4>
<p>By definition,</p>
<p><span class="math display">
\mathbb{P}(E&gt;0.75 \mid B=0.75)
=
\int_{0.75}^{1} f_{E\mid B}(e\mid 0.75)\,de
=
\int_{0.75}^{1} \left[1+\lambda(e-0.5)\right]de.
</span></p>
<p>Compute the two pieces:</p>
<p><span class="math display">
\int_{0.75}^{1} 1\,de = 0.25,
</span></p>
<p>and</p>
<p><span class="math display">
\int_{0.75}^{1} (e-0.5)\,de
=
\left[\frac{1}{2}e^2-\frac{1}{2}e\right]_{0.75}^{1}
=
\left(\frac{1}{2}-\frac{1}{2}\right)
-
\left(\frac{1}{2}\cdot 0.75^2-\frac{1}{2}\cdot 0.75\right).
</span></p>
<p>Since (0.75^2=0.5625),</p>
<p><span class="math display">
\frac{1}{2}\cdot 0.5625-\frac{1}{2}\cdot 0.75
=
0.28125-0.375
=
-0.09375,
</span></p>
<p>so the integral equals (0 - (-0.09375)=0.09375 = ). Therefore,</p>
<p><span class="math display">
\mathbb{P}(E&gt;0.75 \mid B=0.75)
=
0.25 + \lambda\cdot \frac{3}{32}
=
\frac{1}{4}+\frac{3\lambda}{32}.
</span></p>
<p>So the final answer is:</p>
<p><span class="math display">
\boxed{
\mathbb{P}(E&gt;0.75 \mid B=0.75)
=
\frac{1}{4}+\frac{3\lambda}{32}
=
\frac{8+3\lambda}{32}.
}
</span></p>
</section>
<section id="quick-interpretation" class="level4" data-number="6.6.14.4">
<h4 data-number="6.6.14.4" class="anchored" data-anchor-id="quick-interpretation"><span class="header-section-number">6.6.14.4</span> Quick interpretation</h4>
<ul>
<li>If (), then (f_{E,B}(e,b)=1) (independence/uniform on the square), and <span class="math display">
\mathbb{P}(E&gt;0.75 \mid B=0.75)=\frac{1}{4}.
</span></li>
<li>If (&gt;0), then higher bets are associated with higher equity, so conditioning on (B=0.75) increases the probability above (1/4).</li>
<li>If (&lt;0), it decreases the probability below (1/4).</li>
</ul>
<p>(Recall (||) keeps the density nonnegative everywhere.)</p>
</section>
</section>
<section id="bayes-theorem-in-the-continuous-setting" class="level3" data-number="6.6.15">
<h3 data-number="6.6.15" class="anchored" data-anchor-id="bayes-theorem-in-the-continuous-setting"><span class="header-section-number">6.6.15</span> Bayes’ Theorem in the Continuous Setting</h3>
<p>In the continuous setting, Bayes’ theorem relates <strong>prior densities</strong>, <strong>likelihood functions</strong>, and <strong>posterior densities</strong>. Conceptually, nothing has changed from the discrete case: we are still updating beliefs about a hidden variable after observing new evidence. The difference is technical—probabilities are replaced by densities, and sums are replaced by integrals.</p>
</section>
<section id="theorem-bayes-theorem-continuous-version" class="level3" data-number="6.6.16">
<h3 data-number="6.6.16" class="anchored" data-anchor-id="theorem-bayes-theorem-continuous-version"><span class="header-section-number">6.6.16</span> Theorem (Bayes’ Theorem, Continuous Version)</h3>
<p>Let <span class="math inline">(X,Y)</span> be continuous random variables with joint density <span class="math inline">f_{X,Y}(x,y)</span>, marginal density <span class="math inline">f_X(x)</span>, and conditional density <span class="math inline">f_{Y\mid X}(y\mid x)</span>.</p>
<p>If <span class="math inline">f_Y(y)&gt;0</span>, then the conditional density of <span class="math inline">X</span> given <span class="math inline">Y=y</span> is <span class="math display">
f_{X \mid Y}(x \mid y)
=
\frac{f_{Y \mid X}(y \mid x)\,f_X(x)}
{\int f_{Y \mid X}(y \mid x')\,f_X(x')\,dx'}.
</span></p>
<p>This formula expresses the posterior density explicitly in normalized form.</p>
</section>
<section id="the-propto-notation" class="level3" data-number="6.6.17">
<h3 data-number="6.6.17" class="anchored" data-anchor-id="the-propto-notation"><span class="header-section-number">6.6.17</span> The <span class="math inline">\propto</span> Notation</h3>
<p>Bayes’ theorem is often written more compactly as <span class="math display">
f_{X \mid Y}(x \mid y)
\propto
f_{Y \mid X}(y \mid x)\,f_X(x).
</span></p>
<p>The symbol <span class="math inline">\propto</span> means <strong>“is proportional to.”</strong><br>
It indicates that the left-hand side equals the right-hand side <em>up to a normalizing constant</em> that does not depend on <span class="math inline">x</span>.</p>
<p>More precisely, writing <span class="math display">
f_{X \mid Y}(x \mid y) \propto g(x)
</span> means that there exists a constant <span class="math inline">C&gt;0</span> (which may depend on <span class="math inline">y</span>, but not on <span class="math inline">x</span>) such that <span class="math display">
f_{X \mid Y}(x \mid y) = C\, g(x).
</span></p>
<p>The value of <span class="math inline">C</span> is determined by the requirement that a probability density must integrate to one: <span class="math display">
\int f_{X \mid Y}(x \mid y)\,dx = 1.
</span></p>
<p>Solving for <span class="math inline">C</span> yields <span class="math display">
C^{-1} = \int g(x)\,dx,
</span> which recovers the fully normalized version of Bayes’ theorem above.</p>
</section>
<section id="why-this-notation-is-useful" class="level3" data-number="6.6.18">
<h3 data-number="6.6.18" class="anchored" data-anchor-id="why-this-notation-is-useful"><span class="header-section-number">6.6.18</span> Why This Notation Is Useful</h3>
<p>The <span class="math inline">\propto</span> notation emphasizes what <em>changes</em> beliefs rather than what merely rescales them.</p>
<ul>
<li>The <strong>likelihood</strong> <span class="math inline">f_{Y \mid X}(y \mid x)</span> measures how compatible each value of <span class="math inline">x</span> is with the observed data.</li>
<li>The <strong>prior</strong> <span class="math inline">f_X(x)</span> records how plausible each value of <span class="math inline">x</span> was before observing <span class="math inline">y</span>.</li>
<li>The normalizing constant ensures mathematical correctness but does not affect <em>relative plausibility</em> across values of <span class="math inline">x</span>.</li>
</ul>
<p>In many applications—including poker inference and machine learning—the shape of the posterior matters more than its absolute scale. The proportional form makes this clear.</p>
</section>
<section id="example-bayes-theorem-for-equity-given-a-bet" class="level3" data-number="6.6.19">
<h3 data-number="6.6.19" class="anchored" data-anchor-id="example-bayes-theorem-for-equity-given-a-bet"><span class="header-section-number">6.6.19</span> Example: Bayes’ Theorem for Equity Given a Bet</h3>
<p>We now apply Bayes’ theorem to our running continuous poker model.</p>
<p>Recall: - <span class="math inline">E \in [0,1]</span> is the <strong>equity</strong> of a hand (hidden), - <span class="math inline">B \in [0,1)</span> is the <strong>bet size</strong> as a fraction of the pot (observed), - the joint density is <span class="math display">
f_{E,B}(e,b)
=
1 + \lambda(2e-1)(2b-1),
\qquad 0 \le e,b \le 1,
</span> with <span class="math inline">\lambda \in [-1,1]</span>.</p>
<p>From earlier sections, the marginal density of equity is <span class="math display">
f_E(e) = 1,
</span> so equity is uniformly distributed <em>a priori</em>.</p>
</section>
<section id="identifying-the-likelihood" class="level3" data-number="6.6.20">
<h3 data-number="6.6.20" class="anchored" data-anchor-id="identifying-the-likelihood"><span class="header-section-number">6.6.20</span> Identifying the Likelihood</h3>
<p>The likelihood is the conditional density of <span class="math inline">B</span> given <span class="math inline">E=e</span>: <span class="math display">
f_{B \mid E}(b \mid e)
=
\frac{f_{E,B}(e,b)}{f_E(e)}
=
1 + \lambda(2e-1)(2b-1).
</span></p>
<p>This function describes how betting behavior varies with equity in the model.</p>
</section>
<section id="applying-bayes-theorem" class="level3" data-number="6.6.21">
<h3 data-number="6.6.21" class="anchored" data-anchor-id="applying-bayes-theorem"><span class="header-section-number">6.6.21</span> Applying Bayes’ Theorem</h3>
<p>Using the proportional form of Bayes’ theorem, <span class="math display">
f_{E \mid B}(e \mid b)
\propto
f_{B \mid E}(b \mid e)\,f_E(e).
</span></p>
<p>Substituting the expressions above gives <span class="math display">
f_{E \mid B}(e \mid b)
\propto
1 + \lambda(2e-1)(2b-1).
</span></p>
<p>To normalize, we divide by the marginal density of <span class="math inline">B</span>: <span class="math display">
f_B(b)
=
\int_0^1 \bigl[1 + \lambda(2e-1)(2b-1)\bigr]\,de
=
1.
</span></p>
<p>Thus, the normalized posterior density is <span class="math display">
f_{E \mid B}(e \mid b)
=
1 + \lambda(2e-1)(2b-1),
\qquad 0 \le e \le 1.
</span></p>
</section>
<section id="interpretation" class="level3" data-number="6.6.22">
<h3 data-number="6.6.22" class="anchored" data-anchor-id="interpretation"><span class="header-section-number">6.6.22</span> Interpretation</h3>
<ul>
<li>If <span class="math inline">\lambda = 0</span>, then equity and bet size are independent, and observing a bet conveys no information.</li>
<li>If <span class="math inline">\lambda &gt; 0</span> and <span class="math inline">b</span> is large, the posterior density places more weight on higher equity values.</li>
<li>If <span class="math inline">\lambda &gt; 0</span> and <span class="math inline">b</span> is small, the posterior shifts toward lower equity values.</li>
</ul>
<p>In all cases, the cards themselves have not changed.<br>
Only the <em>distribution describing our beliefs</em> has been updated.</p>
</section>
<section id="visualizing-prior-vs-posterior-densities" class="level3" data-number="6.6.23">
<h3 data-number="6.6.23" class="anchored" data-anchor-id="visualizing-prior-vs-posterior-densities"><span class="header-section-number">6.6.23</span> Visualizing Prior vs Posterior Densities</h3>
<p>In our running model, the <strong>prior density</strong> for equity is uniform: <span class="math display">
f_E(e)=1,\qquad 0\le e\le 1.
</span> After observing a bet size <span class="math inline">B=b</span>, the <strong>posterior density</strong> becomes <span class="math display">
f_{E\mid B}(e\mid b)=1+\lambda(2e-1)(2b-1).
</span></p>
<p>The following figure overlays the prior and posterior densities to show how observing a bet reshapes belief about equity.</p>
<div class="cell">
<div class="cell-output-display">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><img src="05-conditional-probability-and-bayes-rule_files/figure-html/unnamed-chunk-2-1.png" class="img-fluid figure-img" width="672"></p>
<figcaption>Prior density f_E(e) (uniform) compared with posterior densities f_{E|B}(e|b) for two observed bet sizes b, using the joint model f(e,b)=1+λ(2e-1)(2b-1).</figcaption>
</figure>
</div>
</div>
</div>
</section>
<section id="conceptual-summary" class="level3" data-number="6.6.24">
<h3 data-number="6.6.24" class="anchored" data-anchor-id="conceptual-summary"><span class="header-section-number">6.6.24</span> Conceptual Summary</h3>
<p>Bayes’ theorem in the continuous setting performs the same task as in the discrete case:</p>
<ul>
<li>it combines prior beliefs with a model of how evidence is generated,</li>
<li>it produces a posterior density reflecting updated information,</li>
<li>it reshapes uncertainty without eliminating it.</li>
</ul>
<p>The <span class="math inline">\propto</span> notation highlights this reweighting process directly. The mathematics is unchanged; only the bookkeeping differs.</p>
</section>
<section id="conceptual-summary-1" class="level3" data-number="6.6.25">
<h3 data-number="6.6.25" class="anchored" data-anchor-id="conceptual-summary-1"><span class="header-section-number">6.6.25</span> Conceptual Summary</h3>
<p>In the continuous setting:</p>
<ul>
<li><strong>Marginal densities</strong> describe baseline uncertainty.</li>
<li><strong>Conditional densities</strong> describe uncertainty after observing evidence.</li>
<li>Conditioning reshapes probability mass without collapsing uncertainty.</li>
</ul>
<p>This is the continuous analogue of belief updating in the discrete running example. The mathematics is the same; only the bookkeeping has changed.</p>
</section>
<section id="bayes-theorem-with-continuous-likelihoods" class="level3" data-number="6.6.26">
<h3 data-number="6.6.26" class="anchored" data-anchor-id="bayes-theorem-with-continuous-likelihoods"><span class="header-section-number">6.6.26</span> Bayes’ Theorem with Continuous Likelihoods</h3>
<p><em>Running Example 2 (continued).</em><br>
A very large bet may lie near the peak of the strong-hand density and far in the tail of the weak-hand density. This difference, not the absolute size of the bet, drives belief updating.</p>
<table class="caption-top table">
<colgroup>
<col style="width: 4%">
</colgroup>
<tbody>
<tr class="odd">
<td>Observing a large bet shifts the posterior density toward stronger hands, but does not identify a single hand. Belief updating remains probabilistic.</td>
</tr>
</tbody>
</table>
</section>
<section id="synthesis-sums-integrals-and-information" class="level3" data-number="6.6.27">
<h3 data-number="6.6.27" class="anchored" data-anchor-id="synthesis-sums-integrals-and-information"><span class="header-section-number">6.6.27</span> Synthesis: Sums, Integrals, and Information</h3>
<p>Across all cases, the logic is the same:</p>
<ul>
<li>hidden variables encode unobserved states,</li>
<li>observable variables encode evidence,</li>
<li>conditional probability updates beliefs coherently.</li>
</ul>
<p>The difference between sums and integrals reflects how uncertainty is represented, not how inference works.</p>
<p>Poker provides a setting in which both discrete and continuous uncertainty arise naturally, making these distinctions concrete and unavoidable.</p>
</section>
</section>
<section id="beyond-poker-bayesian-updating-in-machine-learning" class="level2" data-number="6.7">
<h2 data-number="6.7" class="anchored" data-anchor-id="beyond-poker-bayesian-updating-in-machine-learning"><span class="header-section-number">6.7</span> Beyond Poker: Bayesian Updating in Machine Learning</h2>
<p>Poker is a game of inference under uncertainty. Machine learning, at its core, is the same problem—scaled up, automated, and repeated millions of times.</p>
<p>In both settings, the central task is not prediction alone, but <strong>belief revision</strong>: how should a model’s internal representation of the world change when new data arrives?</p>
<p>Bayesian statistics provides one of the most coherent answers to that question.</p>
<hr>
<section id="models-as-beliefs-data-as-evidence" class="level3" data-number="6.7.1">
<h3 data-number="6.7.1" class="anchored" data-anchor-id="models-as-beliefs-data-as-evidence"><span class="header-section-number">6.7.1</span> Models as Beliefs, Data as Evidence</h3>
<p>In poker, we distinguished between: - a <strong>hidden state</strong> (an opponent’s hand), and - <strong>observable evidence</strong> (bets, timing, revealed cards).</p>
<p>Machine learning systems make the same distinction, though the language is different.</p>
<ul>
<li>The <em>hidden state</em> may be a model parameter, a latent variable, or an unobserved class label.</li>
<li>The <em>observable evidence</em> is data: images, text, clicks, sensor readings.</li>
</ul>
<p>A Bayesian model treats its parameters as random variables. Before seeing data, it assigns a <strong>prior distribution</strong> expressing initial uncertainty. After seeing data, it computes a <strong>posterior distribution</strong> expressing updated beliefs.</p>
<p>The update rule is the same one used at the poker table: <span class="math display">
\text{posterior} \propto \text{likelihood} \times \text{prior}.
</span></p>
<hr>
</section>
<section id="a-concrete-example-learning-to-recognize-handwritten-digits" class="level3" data-number="6.7.2">
<h3 data-number="6.7.2" class="anchored" data-anchor-id="a-concrete-example-learning-to-recognize-handwritten-digits"><span class="header-section-number">6.7.2</span> A Concrete Example: Learning to Recognize Handwritten Digits</h3>
<p>Imagine a system designed to recognize handwritten digits. Each image is noisy: strokes vary in thickness, orientation, and style. No single pixel determines the digit.</p>
<p>A Bayesian classifier might proceed as follows: - The hidden variable is the true digit (0 through 9). - The observable variable is the pixel data in the image. - The likelihood measures how compatible a particular image is with each digit. - The prior encodes how frequently each digit appears in the data set.</p>
<p>When a new image arrives, the system does not simply guess the most similar template. Instead, it updates a distribution over possible digits, weighting each by how consistent it is with the observed image.</p>
<p>This is structurally identical to updating beliefs about an opponent’s hand after observing a bet. The system does not “know” the digit any more than a poker player knows the cards. It maintains and updates uncertainty.</p>
<hr>
</section>
<section id="sequential-learning-from-one-data-point-to-many" class="level3" data-number="6.7.3">
<h3 data-number="6.7.3" class="anchored" data-anchor-id="sequential-learning-from-one-data-point-to-many"><span class="header-section-number">6.7.3</span> Sequential Learning: From One Data Point to Many</h3>
<p>Machine learning systems rarely update beliefs once. They update repeatedly.</p>
<p>Each new data point plays the role of a new betting action in poker. The posterior after one observation becomes the prior for the next: <span class="math display">
\mathbb{P}(\theta \mid x_1, x_2)
\propto
\mathbb{P}(x_2 \mid \theta)\,\mathbb{P}(\theta \mid x_1).
</span></p>
<p>This recursive structure is not an approximation or a heuristic. It is a direct consequence of conditional probability.</p>
<p>In poker, this process unfolds street by street. In machine learning, it unfolds data point by data point.</p>
<hr>
</section>
<section id="continuous-parameters-and-integrals-in-practice" class="level3" data-number="6.7.4">
<h3 data-number="6.7.4" class="anchored" data-anchor-id="continuous-parameters-and-integrals-in-practice"><span class="header-section-number">6.7.4</span> Continuous Parameters and Integrals in Practice</h3>
<p>Many machine learning models involve continuous parameters: - weights in a neural network, - coefficients in a regression model, - latent factors in recommendation systems.</p>
<p>In these settings, Bayesian updating requires <strong>integrating over parameter space</strong>, just as we integrated over continuous hidden variables in the poker models.</p>
<p>The challenge is computational, not conceptual. Exact integrals are often infeasible, so algorithms approximate them: - Markov chain Monte Carlo methods simulate draws from the posterior. - Variational methods approximate the posterior with a simpler distribution. - Particle filters propagate weighted samples over time.</p>
<p>Each of these methods is a response to the same question:<br>
<em>How do we update beliefs when exact calculation is impossible?</em></p>
<p>Poker players face the same issue informally. They approximate belief updates using experience, intuition, and heuristics. Machine learning systems do so algorithmically.</p>
<hr>
</section>
<section id="prediction-uncertainty-and-decision-making" class="level3" data-number="6.7.5">
<h3 data-number="6.7.5" class="anchored" data-anchor-id="prediction-uncertainty-and-decision-making"><span class="header-section-number">6.7.5</span> Prediction, Uncertainty, and Decision-Making</h3>
<p>A key advantage of Bayesian machine learning is that it does not merely produce predictions; it produces <strong>distributions</strong>.</p>
<p>Rather than saying “this image is a 7,” a Bayesian system can say: - there is a 70% chance it is a 7, - a 20% chance it is a 1, - and a 10% chance it is something else.</p>
<p>This distinction matters when decisions have consequences. Medical diagnosis, autonomous vehicles, and fraud detection all require reasoning under uncertainty, not just point estimates.</p>
<p>Poker again provides the analogy. Good decisions are not those that always succeed, but those that perform well <em>on average</em> under uncertainty. Bayesian models make this uncertainty explicit.</p>
<hr>
</section>
<section id="what-poker-teaches-machine-learningand-vice-versa" class="level3" data-number="6.7.6">
<h3 data-number="6.7.6" class="anchored" data-anchor-id="what-poker-teaches-machine-learningand-vice-versa"><span class="header-section-number">6.7.6</span> What Poker Teaches Machine Learning—and Vice Versa</h3>
<p>Poker reminds us that inference is always made with incomplete information, that evidence is noisy, and that certainty is rare. Machine learning reminds us that belief updating can be formalized, automated, and scaled.</p>
<p>Both rely on the same mathematical spine: - conditional probability, - likelihoods, - priors and posteriors, - coherent belief revision.</p>
<p>Understanding Bayesian reasoning in poker is not a parlor trick. It is preparation for understanding how modern systems learn from data, adapt to new information, and make decisions in uncertain environments.</p>
<p>The mathematics developed in this chapter is not specialized. It is foundational.</p>
</section>
</section>
<section id="liberal-arts-sidebar-conditional-probability-and-the-justice-system" class="level2" data-number="6.8">
<h2 data-number="6.8" class="anchored" data-anchor-id="liberal-arts-sidebar-conditional-probability-and-the-justice-system"><span class="header-section-number">6.8</span> Liberal Arts Sidebar: Conditional Probability and the Justice System</h2>
<p>Probability enters the justice system in quiet but consequential ways. Judges, juries, lawyers, and policymakers routinely reason under uncertainty: about guilt, risk, intent, and future behavior. Conditional probability—the mathematics of belief updating—lies at the heart of this reasoning, whether or not it is named explicitly.</p>
<p>Understanding how conditional probability works, and how it can fail, is therefore not merely a technical exercise. It is a civic one.</p>
<hr>
<section id="evidence-is-not-guilt" class="level3" data-number="6.8.1">
<h3 data-number="6.8.1" class="anchored" data-anchor-id="evidence-is-not-guilt"><span class="header-section-number">6.8.1</span> Evidence Is Not Guilt</h3>
<p>A central task in the justice system is to reason from evidence to conclusions. This is precisely the structure we encountered in poker:</p>
<ul>
<li>the <strong>hidden state</strong> is whether a defendant committed a crime,</li>
<li>the <strong>observable evidence</strong> consists of testimony, forensic results, or behavioral patterns.</li>
</ul>
<p>The crucial distinction is between two very different probabilities:</p>
<ul>
<li>the probability of seeing the evidence <em>given</em> guilt, and<br>
</li>
<li>the probability of guilt <em>given</em> the evidence.</li>
</ul>
<p>These quantities are often conflated in public discourse, but they are not the same. Conditional probability exists precisely to keep them distinct.</p>
<p>A piece of evidence may be highly consistent with guilt while still failing to make guilt likely, once background rates and alternative explanations are taken into account.</p>
<hr>
</section>
<section id="base-rates-and-the-prosecutors-fallacy" class="level3" data-number="6.8.2">
<h3 data-number="6.8.2" class="anchored" data-anchor-id="base-rates-and-the-prosecutors-fallacy"><span class="header-section-number">6.8.2</span> Base Rates and the Prosecutor’s Fallacy</h3>
<p>One of the most studied probabilistic errors in legal reasoning is the <em>prosecutor’s fallacy</em>. It occurs when a conditional probability is reversed.</p>
<p>For example, suppose a forensic test has a false positive rate of 1%. It is tempting to argue that a positive test result implies a 99% chance of guilt. But this conclusion silently assumes that guilt was already likely before the test was conducted.</p>
<p>Bayes’ theorem makes the missing ingredient explicit: the <strong>prior probability</strong>. If the prior probability of guilt is very low—as it often is when screening large populations—then even highly accurate evidence may fail to produce a high posterior probability of guilt.</p>
<p>The mathematics does not undermine the value of evidence. It clarifies how evidence should be weighed.</p>
<hr>
</section>
<section id="risk-assessment-and-conditional-predictions" class="level3" data-number="6.8.3">
<h3 data-number="6.8.3" class="anchored" data-anchor-id="risk-assessment-and-conditional-predictions"><span class="header-section-number">6.8.3</span> Risk Assessment and Conditional Predictions</h3>
<p>Conditional probability also plays a growing role in pretrial detention, sentencing, and parole decisions through the use of risk assessment tools.</p>
<p>These tools attempt to estimate quantities such as: - the probability of reoffending <em>given</em> certain background variables, - the probability of failing to appear in court <em>given</em> past behavior.</p>
<p>Mathematically, these are conditional probabilities. Socially, they are judgments that affect liberty.</p>
<p>The challenge is not that conditional probability is being used, but that its assumptions are often hidden: - What data are being conditioned on? - What populations define the base rates? - What outcomes are being predicted, and over what time horizon?</p>
<p>As in poker, belief updating is only as good as the model that connects hidden states to observable data.</p>
<hr>
</section>
<section id="individual-cases-and-group-statistics" class="level3" data-number="6.8.4">
<h3 data-number="6.8.4" class="anchored" data-anchor-id="individual-cases-and-group-statistics"><span class="header-section-number">6.8.4</span> Individual Cases and Group Statistics</h3>
<p>A persistent tension in legal reasoning arises from the difference between <strong>group-level probabilities</strong> and <strong>individual-level decisions</strong>.</p>
<p>Conditional probabilities are typically estimated from populations. Court decisions, however, apply to individuals. The mathematical transition from population-level inference to individual judgment is not automatic.</p>
<p>This tension mirrors a familiar poker dilemma: population tendencies inform expectations, but they do not determine the outcome of any single hand. Conditional probability helps quantify uncertainty, but it does not eliminate it.</p>
<p>Recognizing this limitation is part of responsible probabilistic reasoning.</p>
<hr>
</section>
<section id="transparency-interpretation-and-trust" class="level3" data-number="6.8.5">
<h3 data-number="6.8.5" class="anchored" data-anchor-id="transparency-interpretation-and-trust"><span class="header-section-number">6.8.5</span> Transparency, Interpretation, and Trust</h3>
<p>Unlike poker, the justice system cannot treat probability as a private tool. Decisions must be explained, defended, and scrutinized.</p>
<p>Conditional probability can support transparency when used carefully: - by making assumptions explicit, - by distinguishing evidence from conclusions, - by clarifying how much uncertainty remains.</p>
<p>It can also erode trust when used opaquely, or when numerical outputs are mistaken for objective truth rather than conditional inference.</p>
<p>Mathematics does not resolve ethical questions on its own. But it can sharpen them.</p>
<hr>
</section>
<section id="a-shared-structure" class="level3" data-number="6.8.6">
<h3 data-number="6.8.6" class="anchored" data-anchor-id="a-shared-structure"><span class="header-section-number">6.8.6</span> A Shared Structure</h3>
<p>The mathematics developed in this chapter—conditional probability, Bayes’ theorem, belief updating—does not belong to poker, or to statistics, or to machine learning alone.</p>
<p>It describes a universal structure: - evidence arrives, - beliefs change, - decisions follow under uncertainty.</p>
<p>Whether the stakes are chips, data, or human freedom, the underlying logic is the same. Understanding that logic is one step toward using it responsibly.</p>
</section>
</section>
<section id="homework-problems" class="level2" data-number="6.9">
<h2 data-number="6.9" class="anchored" data-anchor-id="homework-problems"><span class="header-section-number">6.9</span> Homework Problems</h2>
<p>In all problems, state assumptions clearly and justify conclusions mathematically.<br>
When interpreting results, distinguish carefully between probabilities, densities, and decisions.</p>
<hr>
<section id="problems-111-discrete-conditional-probability-and-bayes-theorem" class="level3" data-number="6.9.1">
<h3 data-number="6.9.1" class="anchored" data-anchor-id="problems-111-discrete-conditional-probability-and-bayes-theorem"><span class="header-section-number">6.9.1</span> Problems 1–11: Discrete Conditional Probability and Bayes’ Theorem</h3>
<ol type="1">
<li><p><strong>Conditional probability with cards.</strong><br>
Two cards are drawn without replacement from a standard deck.<br>
Let <span class="math inline">A</span> be the event that the first card is a heart and <span class="math inline">B</span> the event that the second card is a heart.<br>
Compute <span class="math inline">\mathbb{P}(A)</span>, <span class="math inline">\mathbb{P}(B)</span>, and <span class="math inline">\mathbb{P}(A\mid B)</span>.<br>
Explain precisely why <span class="math inline">A</span> and <span class="math inline">B</span> are not independent.</p></li>
<li><p><strong>Explicit joint distribution.</strong><br>
Let <span class="math inline">H\in\{S,M,W\}</span> represent an opponent’s hand category (strong, medium, weak).<br>
Let <span class="math inline">A\in\{\text{bet},\text{check}\}</span> represent an observed action.<br>
Suppose the joint distribution is <span class="math display">
\begin{array}{c|cc}
     &amp; \text{bet} &amp; \text{check} \\ \hline
  S &amp; 0.15 &amp; 0.05 \\
  M &amp; 0.20 &amp; 0.30 \\
  W &amp; 0.10 &amp; 0.20
\end{array}
</span></p>
<ol type="a">
<li>Compute the marginal distributions of <span class="math inline">H</span> and <span class="math inline">A</span>.<br>
</li>
<li>Compute <span class="math inline">\mathbb{P}(H\mid A=\text{bet})</span>.<br>
</li>
<li>Interpret the result in terms of belief updating at the poker table.</li>
</ol></li>
<li><p><strong>Likelihood extraction.</strong><br>
Using the joint distribution in Problem 2, compute<br>
<span class="math inline">\mathbb{P}(A=\text{bet}\mid H=h)</span> for each <span class="math inline">h\in\{S,M,W\}</span>.<br>
Explain how these values encode a betting model.</p></li>
<li><p><strong>Bayesian inversion.</strong><br>
Using your answers to Problems 2 and 3, recompute<br>
<span class="math inline">\mathbb{P}(H\mid A=\text{bet})</span> using Bayes’ theorem in the form <span class="math display">
\mathbb{P}(H=h\mid A)=
\frac{\mathbb{P}(A\mid H=h)\mathbb{P}(H=h)}{\mathbb{P}(A)}.
</span> Verify that the two methods agree.</p></li>
<li><p><strong>Connection to pot odds.</strong><br>
Suppose that after observing a bet, a player estimates<br>
<span class="math display">
\mathbb{P}(H=W\mid A=\text{bet})=0.25.
</span> The pot is $100 and the opponent bets $50.<br>
Assuming that a weak hand always loses at showdown and a non-weak hand always wins, determine whether calling has positive expected value.<br>
Explain how Bayes’ theorem enters this calculation.</p></li>
<li><p><strong>Law of total probability (proof with interpretation).</strong><br>
Let <span class="math inline">H\in\{S,M,W\}</span> be a hidden variable and <span class="math inline">A</span> an observable action.<br>
Prove that <span class="math display">
\mathbb{P}(A=\text{bet})
=
\sum_{h\in\{S,M,W\}} \mathbb{P}(A=\text{bet}\mid H=h)\mathbb{P}(H=h).
</span> Explain why this identity must hold in any model of betting behavior.</p></li>
<li><p><strong>Two-stage betting.</strong><br>
Let <span class="math inline">A_1</span> be a flop action and <span class="math inline">A_2</span> a turn action, each taking values {bet, check}.<br>
Assume <span class="math display">
\mathbb{P}(A_1,A_2\mid H=h)
=
\mathbb{P}(A_1\mid H=h)\mathbb{P}(A_2\mid H=h).
</span></p>
<ol type="a">
<li>Write an expression for <span class="math inline">\mathbb{P}(H\mid A_1,A_2)</span>.<br>
</li>
<li>Explain the modeling assumption being made and give a poker situation where it might fail.</li>
</ol></li>
<li><p><strong>Changing priors.</strong><br>
Recompute Problem 2 assuming the prior on <span class="math inline">H</span> is<br>
<span class="math display">
\mathbb{P}(S)=0.40,\quad \mathbb{P}(M)=0.40,\quad \mathbb{P}(W)=0.20,
</span> while keeping the same conditional probabilities <span class="math inline">\mathbb{P}(A\mid H)</span>.<br>
Compare the new posterior to the original one and explain the difference.</p></li>
<li><p><strong>Non-poker application: diagnostic testing.</strong><br>
A disease occurs in <span class="math inline">2\%</span> of a population. A test has<br>
<span class="math inline">\mathbb{P}(+\mid\text{disease})=0.95</span> and<br>
<span class="math inline">\mathbb{P}(-\mid\text{no disease})=0.90</span>.<br>
Compute the probability that a person has the disease given a positive test.<br>
Identify the prior, likelihood, and posterior explicitly.</p></li>
<li><p><strong>Proof of normalization.</strong><br>
Let <span class="math inline">H</span> be a discrete hidden variable and <span class="math inline">A</span> an observable event with <span class="math inline">\mathbb{P}(A)&gt;0</span>.<br>
Prove directly from Bayes’ theorem that <span class="math display">
\sum_h \mathbb{P}(H=h\mid A)=1.
</span></p></li>
<li><p><strong>Model comparison.</strong><br>
Two betting models assign different values to <span class="math inline">\mathbb{P}(A=\text{bet}\mid H=h)</span>.<br>
Using a fixed prior on <span class="math inline">H</span>, explain how and why these models can lead to different posterior beliefs after the same observed action.</p></li>
</ol>
</section>
<section id="problems-1217-continuous-conditional-probability-and-bayes-theorem" class="level3" data-number="6.9.2">
<h3 data-number="6.9.2" class="anchored" data-anchor-id="problems-1217-continuous-conditional-probability-and-bayes-theorem"><span class="header-section-number">6.9.2</span> Problems 12–17: Continuous Conditional Probability and Bayes’ Theorem</h3>
<ol start="12" type="1">
<li><p><strong>Marginal densities (explicit computation).</strong><br>
Let <span class="math inline">(E,B)</span> have joint density <span class="math display">
f_{E,B}(e,b)=2e,\qquad 0\le b\le e\le 1.
</span></p>
<ol type="a">
<li>Compute the marginal densities <span class="math inline">f_E(e)</span> and <span class="math inline">f_B(b)</span>.<br>
</li>
<li>Interpret each marginal in the context of equity and bet size.</li>
</ol></li>
<li><p><strong>Conditional density.</strong><br>
Using the joint density in Problem 12, compute <span class="math inline">f_{E\mid B}(e\mid b)</span>.<br>
Describe how the support of this density reflects the betting model.</p></li>
<li><p><strong>Polynomial dependence model.</strong><br>
Let <span class="math display">
f_{E,B}(e,b)=1+\lambda(2e-1)(2b-1),\qquad 0\le e,b\le 1,
</span> with <span class="math inline">\lambda\in[-1,1]</span>.</p>
<ol type="a">
<li>Compute <span class="math inline">f_E(e)</span> and <span class="math inline">f_B(b)</span>.<br>
</li>
<li>Compute <span class="math inline">f_{E\mid B}(e\mid b)</span>.</li>
</ol></li>
<li><p><strong>Posterior expectation.</strong><br>
For the model in Problem 14, compute <span class="math inline">\mathbb{E}[E\mid B=b]</span>.<br>
Show explicitly how this expectation depends on <span class="math inline">b</span> and <span class="math inline">\lambda</span>.</p></li>
<li><p><strong>From inference to EV.</strong><br>
Suppose calling a bet yields payoff <span class="math inline">+P</span> if <span class="math inline">E&gt;1/2</span> and payoff <span class="math inline">-C</span> otherwise.<br>
Express the expected value of calling in terms of the posterior density <span class="math inline">f_{E\mid B}(e\mid b)</span>.<br>
Explain how this links Bayesian inference to decision-making.</p></li>
<li><p><strong>Continuous Bayes (proof).</strong><br>
Starting from the definition of conditional density, prove that <span class="math display">
f_{E\mid B}(e\mid b)
=
\frac{f_{B\mid E}(b\mid e)f_E(e)}
     {\int_0^1 f_{B\mid E}(b\mid e')f_E(e')\,de'}.
</span></p></li>
<li><p><strong>Alternative betting model.</strong><br>
Propose a joint density on <span class="math inline">[0,1]^2</span> in which very large bets occur both with very high and very low equity. Describe qualitatively how this would affect posterior inference.</p></li>
<li><p><strong>Discretization vs continuity.</strong><br>
Discretize the equity variable in Problem 14 into three categories and compare the resulting posterior to the continuous one. Discuss what information is lost.</p></li>
<li><p><strong>Synthesis problem.</strong><br>
Explain how conditional probability, Bayes’ theorem, pot odds, and expected value combine into a single framework for reasoning under uncertainty.</p></li>
</ol>
</section>
</section>
<section id="extension-activity" class="level2" data-number="6.10">
<h2 data-number="6.10" class="anchored" data-anchor-id="extension-activity"><span class="header-section-number">6.10</span> Extension Activity</h2>
<p>Extend the chapter’s main tool using a small simulation or a short written reflection connecting poker to another domain.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
  window.document.addEventListener("DOMContentLoaded", function (event) {
    const icon = "";
    const anchorJS = new window.AnchorJS();
    anchorJS.options = {
      placement: 'right',
      icon: icon
    };
    anchorJS.add('.anchored');
    const isCodeAnnotation = (el) => {
      for (const clz of el.classList) {
        if (clz.startsWith('code-annotation-')) {                     
          return true;
        }
      }
      return false;
    }
    const onCopySuccess = function(e) {
      // button target
      const button = e.trigger;
      // don't keep focus
      button.blur();
      // flash "checked"
      button.classList.add('code-copy-button-checked');
      var currentTitle = button.getAttribute("title");
      button.setAttribute("title", "Copied!");
      let tooltip;
      if (window.bootstrap) {
        button.setAttribute("data-bs-toggle", "tooltip");
        button.setAttribute("data-bs-placement", "left");
        button.setAttribute("data-bs-title", "Copied!");
        tooltip = new bootstrap.Tooltip(button, 
          { trigger: "manual", 
            customClass: "code-copy-button-tooltip",
            offset: [0, -8]});
        tooltip.show();    
      }
      setTimeout(function() {
        if (tooltip) {
          tooltip.hide();
          button.removeAttribute("data-bs-title");
          button.removeAttribute("data-bs-toggle");
          button.removeAttribute("data-bs-placement");
        }
        button.setAttribute("title", currentTitle);
        button.classList.remove('code-copy-button-checked');
      }, 1000);
      // clear code selection
      e.clearSelection();
    }
    const getTextToCopy = function(trigger) {
      const outerScaffold = trigger.parentElement.cloneNode(true);
      const codeEl = outerScaffold.querySelector('code');
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
    const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
      text: getTextToCopy
    });
    clipboard.on('success', onCopySuccess);
    if (window.document.getElementById('quarto-embedded-source-code-modal')) {
      const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
        text: getTextToCopy,
        container: window.document.getElementById('quarto-embedded-source-code-modal')
      });
      clipboardModal.on('success', onCopySuccess);
    }
      var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var mailtoRegex = new RegExp(/^mailto:/);
        var filterRegex = new RegExp('/' + window.location.host + '/');
      var isInternal = (href) => {
          return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
      }
      // Inspect non-navigation links and adorn them if external
     var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
      for (var i=0; i<links.length; i++) {
        const link = links[i];
        if (!isInternal(link.href)) {
          // undo the damage that might have been done by quarto-nav.js in the case of
          // links that we want to consider external
          if (link.dataset.originalHref !== undefined) {
            link.href = link.dataset.originalHref;
          }
        }
      }
    function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
      const config = {
        allowHTML: true,
        maxWidth: 500,
        delay: 100,
        arrow: false,
        appendTo: function(el) {
            return el.parentElement;
        },
        interactive: true,
        interactiveBorder: 10,
        theme: 'quarto',
        placement: 'bottom-start',
      };
      if (contentFn) {
        config.content = contentFn;
      }
      if (onTriggerFn) {
        config.onTrigger = onTriggerFn;
      }
      if (onUntriggerFn) {
        config.onUntrigger = onUntriggerFn;
      }
      window.tippy(el, config); 
    }
    const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
    for (var i=0; i<noterefs.length; i++) {
      const ref = noterefs[i];
      tippyHover(ref, function() {
        // use id or data attribute instead here
        let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
        try { href = new URL(href).hash; } catch {}
        const id = href.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note) {
          return note.innerHTML;
        } else {
          return "";
        }
      });
    }
    const xrefs = window.document.querySelectorAll('a.quarto-xref');
    const processXRef = (id, note) => {
      // Strip column container classes
      const stripColumnClz = (el) => {
        el.classList.remove("page-full", "page-columns");
        if (el.children) {
          for (const child of el.children) {
            stripColumnClz(child);
          }
        }
      }
      stripColumnClz(note)
      if (id === null || id.startsWith('sec-')) {
        // Special case sections, only their first couple elements
        const container = document.createElement("div");
        if (note.children && note.children.length > 2) {
          container.appendChild(note.children[0].cloneNode(true));
          for (let i = 1; i < note.children.length; i++) {
            const child = note.children[i];
            if (child.tagName === "P" && child.innerText === "") {
              continue;
            } else {
              container.appendChild(child.cloneNode(true));
              break;
            }
          }
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(container);
          }
          return container.innerHTML
        } else {
          if (window.Quarto?.typesetMath) {
            window.Quarto.typesetMath(note);
          }
          return note.innerHTML;
        }
      } else {
        // Remove any anchor links if they are present
        const anchorLink = note.querySelector('a.anchorjs-link');
        if (anchorLink) {
          anchorLink.remove();
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        if (note.classList.contains("callout")) {
          return note.outerHTML;
        } else {
          return note.innerHTML;
        }
      }
    }
    for (var i=0; i<xrefs.length; i++) {
      const xref = xrefs[i];
      tippyHover(xref, undefined, function(instance) {
        instance.disable();
        let url = xref.getAttribute('href');
        let hash = undefined; 
        if (url.startsWith('#')) {
          hash = url;
        } else {
          try { hash = new URL(url).hash; } catch {}
        }
        if (hash) {
          const id = hash.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          if (note !== null) {
            try {
              const html = processXRef(id, note.cloneNode(true));
              instance.setContent(html);
            } finally {
              instance.enable();
              instance.show();
            }
          } else {
            // See if we can fetch this
            fetch(url.split('#')[0])
            .then(res => res.text())
            .then(html => {
              const parser = new DOMParser();
              const htmlDoc = parser.parseFromString(html, "text/html");
              const note = htmlDoc.getElementById(id);
              if (note !== null) {
                const html = processXRef(id, note);
                instance.setContent(html);
              } 
            }).finally(() => {
              instance.enable();
              instance.show();
            });
          }
        } else {
          // See if we can fetch a full url (with no hash to target)
          // This is a special case and we should probably do some content thinning / targeting
          fetch(url)
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.querySelector('main.content');
            if (note !== null) {
              // This should only happen for chapter cross references
              // (since there is no id in the URL)
              // remove the first header
              if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
                note.children[0].remove();
              }
              const html = processXRef(null, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      }, function(instance) {
      });
    }
        let selectedAnnoteEl;
        const selectorForAnnotation = ( cell, annotation) => {
          let cellAttr = 'data-code-cell="' + cell + '"';
          let lineAttr = 'data-code-annotation="' +  annotation + '"';
          const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
          return selector;
        }
        const selectCodeLines = (annoteEl) => {
          const doc = window.document;
          const targetCell = annoteEl.getAttribute("data-target-cell");
          const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
          const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          const lines = annoteSpan.getAttribute("data-code-lines").split(",");
          const lineIds = lines.map((line) => {
            return targetCell + "-" + line;
          })
          let top = null;
          let height = null;
          let parent = null;
          if (lineIds.length > 0) {
              //compute the position of the single el (top and bottom and make a div)
              const el = window.document.getElementById(lineIds[0]);
              top = el.offsetTop;
              height = el.offsetHeight;
              parent = el.parentElement.parentElement;
            if (lineIds.length > 1) {
              const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
              const bottom = lastEl.offsetTop + lastEl.offsetHeight;
              height = bottom - top;
            }
            if (top !== null && height !== null && parent !== null) {
              // cook up a div (if necessary) and position it 
              let div = window.document.getElementById("code-annotation-line-highlight");
              if (div === null) {
                div = window.document.createElement("div");
                div.setAttribute("id", "code-annotation-line-highlight");
                div.style.position = 'absolute';
                parent.appendChild(div);
              }
              div.style.top = top - 2 + "px";
              div.style.height = height + 4 + "px";
              div.style.left = 0;
              let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
              if (gutterDiv === null) {
                gutterDiv = window.document.createElement("div");
                gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
                gutterDiv.style.position = 'absolute';
                const codeCell = window.document.getElementById(targetCell);
                const gutter = codeCell.querySelector('.code-annotation-gutter');
                gutter.appendChild(gutterDiv);
              }
              gutterDiv.style.top = top - 2 + "px";
              gutterDiv.style.height = height + 4 + "px";
            }
            selectedAnnoteEl = annoteEl;
          }
        };
        const unselectCodeLines = () => {
          const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
          elementsIds.forEach((elId) => {
            const div = window.document.getElementById(elId);
            if (div) {
              div.remove();
            }
          });
          selectedAnnoteEl = undefined;
        };
          // Handle positioning of the toggle
      window.addEventListener(
        "resize",
        throttle(() => {
          elRect = undefined;
          if (selectedAnnoteEl) {
            selectCodeLines(selectedAnnoteEl);
          }
        }, 10)
      );
      function throttle(fn, ms) {
      let throttle = false;
      let timer;
        return (...args) => {
          if(!throttle) { // first call gets through
              fn.apply(this, args);
              throttle = true;
          } else { // all the others get throttled
              if(timer) clearTimeout(timer); // cancel #2
              timer = setTimeout(() => {
                fn.apply(this, args);
                timer = throttle = false;
              }, ms);
          }
        };
      }
        // Attach click handler to the DT
        const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
        for (const annoteDlNode of annoteDls) {
          annoteDlNode.addEventListener('click', (event) => {
            const clickedEl = event.target;
            if (clickedEl !== selectedAnnoteEl) {
              unselectCodeLines();
              const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
              if (activeEl) {
                activeEl.classList.remove('code-annotation-active');
              }
              selectCodeLines(clickedEl);
              clickedEl.classList.add('code-annotation-active');
            } else {
              // Unselect the line
              unselectCodeLines();
              clickedEl.classList.remove('code-annotation-active');
            }
          });
        }
    const findCites = (el) => {
      const parentEl = el.parentElement;
      if (parentEl) {
        const cites = parentEl.dataset.cites;
        if (cites) {
          return {
            el,
            cites: cites.split(' ')
          };
        } else {
          return findCites(el.parentElement)
        }
      } else {
        return undefined;
      }
    };
    var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
    for (var i=0; i<bibliorefs.length; i++) {
      const ref = bibliorefs[i];
      const citeInfo = findCites(ref);
      if (citeInfo) {
        tippyHover(citeInfo.el, function() {
          var popup = window.document.createElement('div');
          citeInfo.cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    }
  });
  </script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="../chapters/04-random-variables-expectation-and-variance.html" class="pagination-link" aria-label="Random Variables, Expectation, and Variance">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Random Variables, Expectation, and Variance</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="../chapters/06-expected-value-in-multi-stage-decisions.html" class="pagination-link" aria-label="Expected Value in Multi-Stage Decisions">
        <span class="nav-page-text"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Expected Value in Multi-Stage Decisions</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->




</body></html>