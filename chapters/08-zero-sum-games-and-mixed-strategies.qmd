---
title: "Zero-Sum Games and Mixed Strategies"
chapter-id: "ch08"
part: "Part III: Game Theory (Intro / Toy Models)"
poker-topics:
  - "11 Zero-Sum Interaction"
  - "12 Mixed Strategies"
applications:
  - "sports strategy (penalty kicks/play calling)"
  - "cybersecurity (attack–defense)"
  - "military strategy (adversarial planning)"
sidebar:
  lens: "History / ethics"
  title: "War, Games, and Human Cost"
  theme: "Game theory’s military origins and critiques of zero-sum framing that can dehumanize opponents or obscure moral stakes."
---
::: {.callout-note}
**Chapter at a glance**

- **Part:** Part III: Game Theory (Intro / Toy Models)
- **Poker topics:** 11 Zero-Sum Interaction, 12 Mixed Strategies
- **Beyond poker:** sports strategy (penalty kicks/play calling), cybersecurity (attack–defense), military strategy (adversarial planning)

:::

## Motivating Example

If you always bluff, you get snapped; if you never bluff, you get overfolded against. Randomizing feels irrational—but can be optimal.

**Guiding question.** Why would a rational strategy ever involve deliberate randomness?

- Situation: If you always bluff, you get snapped; if you never bluff, you get overfolded against. Randomizing feels irrational—but can be optimal.
- Why it matters: this introduces 11 Zero-Sum Interaction, 12 Mixed Strategies.


## Strategic Games Introduction


### Strategic Interaction

Game theory studies situations in which multiple decision-makers interact and the outcome for each depends on the actions of others.

Before introducing formal definitions, we begin with three canonical examples.



### Example 1: Rock–Paper–Scissors

There are two players. Each simultaneously chooses one element of the strategy set

$$
S_i = \{\text{Rock}, \text{Paper}, \text{Scissors}\}.
$$

The outcome is determined by the standard rules:

- Rock defeats Scissors,
- Scissors defeats Paper,
- Paper defeats Rock,
- Identical choices result in a tie.

Assign payoff $+1$ to a win, $-1$ to a loss, and $0$ to a tie.

The payoff matrix to Player I is

$$
A =
\begin{pmatrix}
0 & -1 & 1 \\
1 & 0 & -1 \\
-1 & 1 & 0
\end{pmatrix}.
$$

Player II’s payoff is $-A_{ij}$.

This is a simultaneous two–player zero–sum game.



### Example 2: The Prisoner’s Dilemma

Two players simultaneously choose between

$$
S_i = \{\text{Cooperate}, \text{Defect}\}.
$$

A possible payoff matrix (to Player I) is

$$
A =
\begin{pmatrix}
-1 & -4 \\
0 & -3
\end{pmatrix}.
$$

Here the total payoff

$$
u_1(s_1,s_2) + u_2(s_1,s_2)
$$

is not constant across outcomes. The game is therefore not zero–sum.

This example illustrates that strategic interaction need not be purely adversarial. Incentives may conflict without being perfectly opposed.



### Example 3: A Sequential Poker Decision

Consider a simplified river situation in heads-up poker. The pot is $P$. Player I acts first and chooses **bet** or **check**.

- If Player I checks, the hand proceeds directly to showdown.
- If Player I bets, Player II observes the bet and chooses **call** or **fold**.

Suppose that at showdown Player I wins with probability

$$
x = \Pr(\text{Player I wins at showdown}),
\qquad 0 \le x \le 1.
$$

Player I’s payoffs are:

- If checking:
  $$
  xP.
  $$

- If betting and Player II folds:
  $$
  P.
  $$

- If betting and Player II calls:
  $$
  x(P+B) - (1-x)B.
  $$

Unlike the previous examples, this game is sequential. Player II’s action depends on observing Player I’s move. A strategy must therefore specify behavior conditional on earlier actions.



### What These Examples Share

Despite their differences, each example has common structural elements:

- A set of players,
- A set of possible actions (or strategies) for each player,
- A rule that assigns a payoff to every possible combination of actions.

In Rock–Paper–Scissors and the Prisoner’s Dilemma, actions are chosen simultaneously.

In the poker example, actions occur in sequence, and later choices depend on earlier ones.

The unifying feature is interdependence:

> Each player’s payoff depends not only on their own choice, but on the choices of others.

This distinguishes games from optimization against nature.

In optimization under uncertainty, one chooses $s$ to maximize an expected value such as

$$
\mathbb{E}[X(s)],
$$

where uncertainty arises from random processes. Nature does not adapt.

In a game, by contrast, each participant solves an optimization problem whose objective depends on the simultaneous or sequential optimization of others.



### Formal Definition (Finite Simultaneous Game)

We now abstract the structure illustrated above.

A **finite game** consists of:

1. A finite set of players indexed by $1,\dots,N$.
2. For each player $i$, a finite strategy set $S_i$.
3. For each player $i$, a payoff function
   $$
   u_i : S_1 \times \cdots \times S_N \to \mathbb{R}.
   $$

An element

$$
(s_1,\dots,s_N) \in S_1 \times \cdots \times S_N
$$

is called a **strategy profile**.

The value $u_i(s_1,\dots,s_N)$ represents the payoff to player $i$ when that strategy profile is played.

In simultaneous games, this structure is complete.

In sequential games, the strategy sets consist not merely of single actions but of complete contingent plans — functions defined on possible histories of play. The payoff function is defined on terminal histories rather than directly on single actions.

The next section formalizes this distinction more precisely.

### Elements of a Game

A game is determined not merely by the presence of multiple decision-makers, but by a precise specification of how their decisions interact. We now describe the structural components that define a game.

#### Players

The **players** are the decision-making agents in the model.

Formally, let
$$
\mathcal{N} = \{1,2,\dots,N\}
$$
denote the finite set of players.

Each player is assumed to be rational in the sense that they seek to maximize their own payoff, given their beliefs about the actions of others.

In a heads-up poker hand, for example, $\mathcal{N} = \{1,2\}$ consists of the bettor and the defender. In a multiway pot, $\mathcal{N}$ would contain more elements.

The number of players fundamentally shapes the structure of the game: two-player games often exhibit different mathematical properties from games with three or more participants.

#### Strategy Sets

For each player $i \in \mathcal{N}$, we specify a finite set $S_i$ of available actions, called the **strategy set** of player $i$.

An element $s_i \in S_i$ is called a **pure strategy**.

The term “strategy” may refer to a single move or to a complete plan of action, depending on the timing structure of the game (which we describe below).

In a simplified river poker model:

- Player I might have $S_1 = \{\text{bet}, \text{check}\}$.
- Player II might have $S_2 = \{\text{call}, \text{fold}\}$.

The strategy sets specify what actions are permitted within the model. They are modeling choices, not empirical descriptions of all human behavior.

#### Strategy Profiles

A **strategy profile** is a tuple
$$
(s_1, \dots, s_N) \in S_1 \times \cdots \times S_N.
$$

It specifies one strategy choice for every player.

The Cartesian product
$$
S_1 \times \cdots \times S_N
$$
is the set of all possible strategy  generated by players’ choices.



#### Payoff Functions

For each player $i$, a **payoff function**
$$
u_i : S_1 \times \cdots \times S_N \to \mathbb{R}
$$
assigns a real number to every strategy profile.

The number $u_i(s_1,\dots,s_N)$ represents the payoff to player $i$ when the strategy profile $(s_1,\dots,s_N)$ is played.

Payoffs may represent:

- Monetary winnings,
- Utility,
- Points,
- Or any quantitative measure of preference.


The payoff function encodes the incentives of the game. Once specified, the strategic problem becomes purely mathematical.

#### Timing Structure

The **timing structure** describes when decisions are made and in what order.

Two principal types:

- **Simultaneous-move games**: all players choose without observing others’ actions.
- **Sequential games**: players move in a specified order, possibly observing previous moves.

In a single betting round of poker, one player acts first and the other responds. Across multiple betting rounds, the timing structure becomes more complex.

The timing structure determines whether strategies are single actions or complete contingent plans.


#### Information Structure

The **information structure** of a game specifies what each player knows when choosing a strategy.

Key questions include:

- Do players observe each other’s actions?
- Do players know the payoff functions?
- Do players possess private information?

In simultaneous-move games, players choose without observing the actions of others. In sequential games, later players may observe earlier moves.

In poker, players typically do not observe their opponents’ private cards. This creates informational asymmetry, which profoundly affects strategic reasoning.

Information structure determines what strategies are meaningful and what beliefs are required.

Together, the players, strategy sets, payoff functions, information structure, and timing structure fully specify a game.

Once these elements are fixed, the analysis proceeds by studying how rational players reason within that structure.

## II. A Taxonomy of Games

The definition of a game specifies its formal components: players, strategy sets, payoffs, information, and timing. But games differ profoundly in structure. Some involve pure competition; others allow for cooperation. Some are resolved in a single move; others unfold over time. Some contain no randomness; others incorporate chance.

To analyze games effectively, we must classify them according to structural features that determine their mathematical behavior.



We begin with the most fundamental distinction: the relationship among players’ payoffs.

### Zero–Sum vs. Non–Zero–Sum

The nature of strategic interaction depends critically on how the players’ payoffs are related.

#### Zero–Sum Games

A two-player game is called **zero–sum** if, for every strategy profile $(s_1,s_2)$,

$$
u_1(s_1,s_2) + u_2(s_1,s_2) = 0.
$$

In such games, one player’s gain is exactly the other player’s loss.

All interests are strictly opposed. There is no possibility of mutual benefit. Any increase in one player’s payoff necessarily decreases the other’s payoff by the same amount.

In this setting, it is sufficient to describe a single payoff function. If we define

$$
u(s_1,s_2) = u_1(s_1,s_2),
$$

then Player II’s payoff is simply $-u(s_1,s_2)$.

Zero–sum structure leads to a particularly clean mathematical theory, culminating in the minimax theorem.

#### Constant–Sum Games

A game is **constant–sum** if there exists a constant $C$ such that

$$
u_1(s_1,s_2) + u_2(s_1,s_2) = C
$$

for all strategy profiles.

Constant–sum games are strategically equivalent to zero–sum games. Indeed, if we define

$$
\tilde{u}_1(s_1,s_2) = u_1(s_1,s_2) - \frac{C}{2},
$$

then

$$
\tilde{u}_1(s_1,s_2) + \tilde{u}_2(s_1,s_2) = 0.
$$

Thus, constant–sum games differ from zero–sum games only by an affine transformation of payoffs.

#### General–Sum Games

A game is **general–sum** (or non–zero–sum) if the sum

$$
u_1(s_1,s_2) + u_2(s_1,s_2)
$$

varies across strategy profiles.

In such games, players’ interests are not perfectly opposed. Some outcomes may benefit both players relative to others. Conflict and cooperation may coexist.

This distinction has profound consequences.

In zero–sum games:
- The interaction is one of pure conflict.
- One player’s objective is to minimize the other’s maximum possible gain.
- Equilibrium is characterized by minimax reasoning.

In general–sum games:
- Players may have incentives to coordinate.
- Equilibrium concepts must account for mutual best responses rather than pure opposition.
- The mathematical structure becomes more complex.

Throughout this chapter, we focus on finite two-player zero–sum games. Their structural simplicity allows us to develop a complete and elegant theory before turning to more general strategic environments.

### Simultaneous vs. Sequential Games

Games differ not only in payoffs, but in timing.

The distinction between simultaneous and sequential play changes the internal mathematical structure of the game.



#### Simultaneous-Move Games

In a finite simultaneous game:

- Each player $i$ has a finite strategy set $S_i$.
- A strategy is a single action.
- A strategy profile is an element
  $$
  (s_1, \dots, s_N) \in S_1 \times \cdots \times S_N.
  $$
- The payoff function is
  $$
  u_i : S_1 \times \cdots \times S_N \to \mathbb{R}.
  $$

Timing does not explicitly appear in the structure because players choose without observing the actions of others.

In the two-player case, this structure is naturally represented by a payoff matrix.

Analysis proceeds by examining best responses within this product space.



#### Sequential Games

In a sequential game, actions occur over time. Later players may observe earlier moves before acting.

To formalize this, we introduce **histories**.

A history is a finite sequence of actions:

$$
h = (a_1, a_2, \dots, a_k).
$$

The set of all histories is denoted by $H$, and the empty history $\varnothing$ represents the start of the game.

A subset $Z \subset H$ consists of **terminal histories**, at which the game ends.

Payoffs are assigned to terminal histories:

$$
u_i : Z \to \mathbb{R}.
$$

Each nonterminal history $h \in H \setminus Z$ is assigned to a player $P(h)$ who moves after that history.  
At that point, the player chooses an action from a finite set $A_i(h)$.

Thus the game is represented as a tree of histories.



#### Strategies in Sequential Games

A pure strategy must specify what a player will do at every history where they are called upon to act.

Let

$$
H_i = \{ h \in H \setminus Z : P(h) = i \}.
$$

Then a pure strategy for player $i$ is a function

$$
s_i : H_i \to A
$$

such that

$$
s_i(h) \in A_i(h).
$$

Thus, unlike simultaneous games where a strategy is a single action, in sequential games a strategy is a **complete contingent plan**.



#### Information Structure

If a player cannot distinguish between certain histories, those histories form an **information set**.

Let $\mathcal{I}_i$ denote the collection of information sets for player $i$.

A strategy must assign the same action at all histories within the same information set.  
Equivalently,

$$
s_i : \mathcal{I}_i \to A.
$$

Information therefore restricts which contingent plans are admissible.



#### Strategic Form vs. Extensive Form

Every finite sequential game can be reduced to a strategic-form representation:

$$
u_i : S_1 \times \cdots \times S_N \to \mathbb{R},
$$

where each $S_i$ is the set of admissible contingent plans.

However, the internal structure differs:

- In simultaneous games, strategies are elements of small finite sets.
- In sequential games, strategies are functions defined on histories or information sets.
- The size of $S_i$ may grow exponentially with the depth of the tree.

This structural difference shapes analysis.

Simultaneous games are naturally analyzed using matrices and best-response geometry.

Sequential games invite analysis through backward reasoning along the tree.

The taxonomy of games is therefore not merely descriptive.  
It determines the mathematical tools required for equilibrium analysis.


### Deterministic vs. Stochastic Games

Games also differ in whether randomness enters the model externally.

#### Deterministic Games

A game is **deterministic** if the outcome is determined entirely by the players’ strategy choices.

Formally, once a strategy profile
$$
(s_1, \dots, s_N)
$$
is selected, the payoff vector
$$
(u_1(s_1,\dots,s_N), \dots, u_N(s_1,\dots,s_N))
$$
is fixed.

There are no additional random events influencing the result.

In such games, uncertainty arises only from the strategic choices of other players.

#### Games with Chance Nodes

In many strategic environments, outcomes depend not only on player choices but also on random events.

These games include **chance nodes**, representing moves by “nature.” At such nodes, an outcome is selected according to a specified probability distribution.

Formally, one may model this by enlarging the strategy profile to include a random variable $Z$ governed by a known distribution, so that payoffs take the form
$$
u_i(s_1,\dots,s_N, Z).
$$

Expected payoffs are then computed by integrating over the randomness of $Z$.

Poker provides a natural example. The dealing of cards is a stochastic event. Even if players’ strategies are fixed, the realized payoff depends on the random shuffle.

#### Randomization by Players vs. Randomness Imposed by Nature

It is crucial to distinguish two conceptually different sources of randomness:

1. **Randomness imposed by nature**, such as shuffled cards or dice rolls.
2. **Randomization chosen by players**, such as mixing between strategies.

In the first case, randomness is external to the players’ control. It is part of the environment.

In the second case, randomness is a deliberate strategic device. A player may choose a probability distribution over actions in order to prevent exploitation.

Mathematically, these two types of randomness are treated differently:

- Exogenous randomness affects the payoff function directly.
- Strategic randomization enlarges the strategy set itself.

This distinction becomes central when we introduce mixed strategies. There, probability is not modeling ignorance about events, but representing a conscious strategic choice.

### Fixed Horizon vs. Random Horizon

Games may also differ in how long they last.

The horizon of a game refers to the number of decision stages before termination.



#### Fixed Horizon

In a fixed-horizon game, the number of stages is predetermined.

For example:

- A poker hand has a finite sequence of betting rounds.
- A chess game ends when a terminal position is reached.
- A bargaining game may allow exactly $T$ offers.

Mathematically, we may represent such a game as a sequential game whose histories have bounded length. If $T$ denotes the maximum number of stages, then every terminal history satisfies

$$
\text{length}(h) \le T.
$$

A related formulation is a **repeated game**: the same stage game is played $T$ times.

Let $G$ be a finite strategic-form game with payoff functions $u_i$.  
The $T$-period repeated game consists of:

- The same players,
- The same stage strategy sets in each period,
- Histories consisting of sequences of past action profiles,
- Payoffs given by an aggregation such as

  $$
  U_i = \sum_{t=1}^T u_i(s^t),
  $$

  where $s^t$ denotes the strategy profile played in period $t$.

Thus a fixed-horizon repeated game can be represented as a single sequential game whose histories record past play.

The mathematical distinction lies not in the players or payoffs, but in how histories accumulate across stages.



#### Infinite Horizon

In an infinite-horizon game, there is no predetermined terminal stage.

Histories may be arbitrarily long:

$$
h = (a_1, a_2, a_3, \dots).
$$

To ensure well-defined payoffs, one typically introduces a discount factor $0 < \delta < 1$ and defines

$$
U_i = \sum_{t=1}^\infty \delta^{t-1} u_i(s^t).
$$

Infinite horizon models are central in economics and repeated strategic interaction, where long-term incentives influence current behavior.



#### Random Horizon

In a random-horizon game, termination occurs probabilistically.

At each stage, the game continues with probability $\delta$ and ends with probability $1-\delta$.

The expected payoff can then be written as

$$
U_i = \mathbb{E}\left[ \sum_{t=1}^{\tau} u_i(s^t) \right],
$$

where $\tau$ is a random stopping time.

Notably, the random-horizon model with continuation probability $\delta$ is mathematically equivalent to the infinite-horizon discounted model above.



#### Conceptual Distinction

A fixed-horizon multi-stage game and a $T$-period repeated game of a single stage game can be represented within the same formal framework:

- Histories record prior actions,
- Strategies are functions defined on histories,
- Payoffs aggregate across stages.

The difference is interpretive rather than structural:

- In a multi-stage game, stages may differ in available actions.
- In a repeated game, each stage is strategically identical.

In both cases, horizon length affects incentives.  
When the end is known, backward reasoning may simplify behavior.  
When continuation is uncertain or infinite, future consequences may sustain cooperation or deterrence.

Thus horizon structure alters not the definition of a game, but the strategic logic that emerges from it.

### Information Structure in Games

Games differ not only in payoffs and timing, but in what players know.

To avoid confusion, we separate three distinct sources of uncertainty:

1. **Stochastic uncertainty** — randomness generated by nature.
2. **Imperfect information** — limited observation of past actions.
3. **Incomplete information** — uncertainty about the underlying structure of the game.

These are conceptually and mathematically different.



#### Stochastic Structure (Nature)

Many games include exogenous randomness:

- Card deals in poker,
- Dice rolls,
- Random shocks in economic models.

Formally, certain histories are assigned to **nature**.  
If $P(h) = \text{Nature}$, then the next action is drawn from a known probability distribution

$$
\pi(\cdot \mid h).
$$

All players know this distribution, and this knowledge is common knowledge.

Randomness therefore affects expected payoffs, but it does not by itself create incomplete information.

A game may be stochastic and still have complete information.



#### Perfect vs. Imperfect Information

This distinction concerns what players observe about the history of play.

A sequential game has **perfect information** if, whenever a player moves, they observe the entire prior history.

Equivalently, every information set contains a single history.

Chess is a perfect-information game.

A game has **imperfect information** if some player moves without observing all prior actions.

Formally, some information set contains multiple histories.

Poker is imperfect-information because players do not observe opponents’ private cards.

Imperfect information alters the structure of admissible strategies, since a strategy must assign the same action at all histories within an information set.



#### Complete vs. Incomplete Information

This distinction concerns knowledge of the game’s primitives.

A game has **complete information** if:

- All players know the strategy sets,
- All players know the payoff functions,
- All players know the probability laws governing nature,
- This knowledge is common knowledge.

In standard poker:

- The rules are known,
- The payoff structure is known,
- The distribution of cards is known.

Although players do not know opponents’ private cards, the game is one of complete information. The uncertainty is stochastic and observational, not structural.

A game has **incomplete information** if at least one player lacks knowledge about:

- Another player’s payoff function,
- Another player’s available strategies,
- Or some underlying parameter of the model.

In such cases, players must form beliefs not only about actions, but about the structure generating those actions.



#### Independence of the Distinctions

These categories are logically independent.

A game may be:

- Perfect and complete (e.g., chess),
- Imperfect and complete (e.g., standard poker),
- Perfect and incomplete (e.g., bargaining with unknown valuations but observable actions),
- Imperfect and incomplete (e.g., poker with private risk preferences).

Randomness, observation, and structural knowledge operate at different levels of the model.

Maintaining these distinctions is essential.  
Random events affect expected value.  
Imperfect information restricts admissible strategies.  
Incomplete information enlarges the strategic problem by introducing beliefs about unknown parameters.


Later chapters will formalize incomplete information using type spaces and Bayesian reasoning. For now, it is enough to recognize that these are distinct dimensions along which games vary.

### Cooperative vs. Competitive Games

Games may also be classified according to whether players are permitted to coordinate and form binding agreements.

#### Competitive (Noncooperative) Games

In a competitive or **noncooperative** game:

- Each player chooses strategies independently.
- Agreements, if discussed, are not enforceable.
- The outcome is determined solely by the strategy profile
  $$
  (s_1, \dots, s_N).
  $$

Most of this chapter studies competitive games.  
Poker, Rock–Paper–Scissors, and the Prisoner’s Dilemma (in its standard formulation) are noncooperative games.

The mathematical framework is based on:

- Strategy sets,
- Payoff functions,
- Equilibrium defined through mutual best responses.



#### Cooperative Games

In a **cooperative** game:

- Players may form binding agreements.
- Coalitions may form.
- The central question becomes how collective gains are distributed among members.

Rather than focusing on individual strategy profiles, cooperative game theory studies:

- The value generated by coalitions,
- Stability of agreements,
- Allocation rules.

Formally, a cooperative game is often described by a function

$$
v : 2^N \to \mathbb{R},
$$

where $v(S)$ represents the value that coalition $S \subseteq N$ can guarantee for itself.




#### Poker Perspective

Poker is modeled as a noncooperative game: each player acts independently to maximize their own expected value.

However, tournament poker reveals the boundary between cooperative and competitive modeling. If two players agree to soft-play one another or share prize money regardless of finish, they are effectively forming a coalition. Such coordination alters the payoff structure and invalidates the standard noncooperative analysis.

This illustrates an important modeling principle:

The distinction between cooperative and competitive games is not about whether cooperation is possible in practice. It concerns whether cooperation is built into the mathematical model.


This chapter focuses on competitive (noncooperative) games.  
Cooperative game theory forms a parallel and equally rich branch of the subject.



## III. Finite Two–Player Simultaneous Zero–Sum Games

We now restrict attention to a specific class of games whose structure allows a complete mathematical theory: finite, two-player, simultaneous-move, zero–sum games.

### Formal Definition

A **finite two–player zero–sum game** consists of:

1. Two players, labeled Player I and Player II.
2. Finite strategy sets
   $$
   S_1 = \{s_1^1, \dots, s_1^m\}, 
   \quad
   S_2 = \{s_2^1, \dots, s_2^n\}.
   $$
3. A payoff function
   $$
   u : S_1 \times S_2 \to \mathbb{R}
   $$
   representing the payoff to Player I.

The payoff to Player II is defined by
$$
- u(s_1, s_2).
$$

Thus, for every strategy pair $(s_1, s_2)$,
$$
u_1(s_1,s_2) + u_2(s_1,s_2) = 0.
$$

The game is simultaneous: both players choose their strategies without observing the choice of the other.

Because the strategy sets are finite, the Cartesian product
$$
S_1 \times S_2
$$
contains finitely many outcomes.

Once the function $u$ is specified, the strategic problem is completely determined.

### Matrix Representation

Since the strategy sets are finite, the game may be represented by a matrix.

Index Player I’s strategies as
$$
s_1^1, \dots, s_1^m,
$$
and Player II’s strategies as
$$
s_2^1, \dots, s_2^n.
$$

Define the matrix
$$
A \in \mathbb{R}^{m \times n}
$$
by
$$
A_{ij} = u(s_1^i, s_2^j).
$$

Interpretation:

- Each **row** corresponds to a strategy of Player I.
- Each **column** corresponds to a strategy of Player II.
- The entry $A_{ij}$ is the payoff to Player I when Player I chooses $s_1^i$ and Player II chooses $s_2^j$.

Player II’s payoff is $-A_{ij}$.

Thus, a finite two–player zero–sum game is completely characterized by a real matrix.

The analysis of such games reduces to studying the strategic properties of this matrix.

### Worked Example: Building the Payoff Matrix for the Three-Card Poker Game

We now illustrate, in full detail, how a payoff matrix is constructed when the game includes private cards and simultaneous betting decisions.

#### Game Rules

- Deck: $\{A,K,Q\}$ with $A>K>Q$.
- Nature deals one card to each player **without replacement**, so the six ordered deals are
  $$
  (A,K),(A,Q),(K,A),(K,Q),(Q,A),(Q,K),
  $$
  each with probability $1/6$.
- After seeing their own card, each player simultaneously chooses either **Bet** ($B$) or **No bet** ($N$).
- Payoffs to Player I are:

  1. If both choose $N$, go to showdown:
     - higher card wins $+1$,
     - lower card loses $-1$.

  2. If exactly one player bets, the bettor wins $+1$ and the other loses $-1$.

  3. If both bet, go to showdown:
     - higher card wins $+2$,
     - lower card loses $-2$.

This is a two-player zero–sum game.



#### Step 1: Pure Strategies Are Contingent Plans

A pure strategy is not a card. A card is chosen by nature.

A pure strategy for a player is a rule that specifies what to do holding each possible card:

$$
s:\{A,K,Q\}\to\{N,B\}.
$$

Since there are 2 choices for each of 3 cards, there are

$$
2^3 = 8
$$

pure strategies.

We encode a strategy by a triple indicating the action with $(A,K,Q)$, in that order.
For example:

- $NBN$ means: no bet with $A$, bet with $K$, no bet with $Q$.
- $BBB$ means: bet with all cards.

We will use the ordering:

$$
NNN,\ NNB,\ NBN,\ NBB,\ BNN,\ BNB,\ BBN,\ BBB.
$$

Let these be denoted
$$
s^1, s^2, \dots, s^8.
$$



#### Step 2: Define the Payoff Matrix Entrywise

The payoff matrix $A$ is an $8\times 8$ matrix where

$$
A_{ij} = u(s^i, s^j)
$$

is the **expected payoff to Player I** when Player I uses strategy $s^i$ and Player II uses strategy $s^j$.

Because the cards are dealt randomly, each entry is an expectation over the six feasible deals:

$$
A_{ij}
=
\frac{1}{6}
\sum_{(c_1,c_2)}
\text{Payoff}_I\big(c_1,c_2;\ s^i(c_1),\ s^j(c_2)\big),
$$

where the sum runs over

$$
(A,K),(A,Q),(K,A),(K,Q),(Q,A),(Q,K).
$$

Notice what has happened structurally:

- The *impossible* outcomes $(A,A)$, $(K,K)$, $(Q,Q)$ never appear.
- They are not assigned payoff $0$.
- They simply have probability $0$ and are absent from the expectation.



#### Step 3: Compute One Matrix Entry Explicitly

Compute $A_{1,6}$ where:

- Player I uses $s^1 = NNN$ (never bets),
- Player II uses $s^6 = BNB$ (bets with $A$, does not bet with $K$, bets with $Q$).

We evaluate Player I’s payoff deal by deal.

1. Deal $(A,K)$:
   - I holds $A$ and plays $N$.
   - II holds $K$ and plays $N$.
   - Both $N$ $\Rightarrow$ showdown, $A$ wins.
   - Payoff to I: $+1$.

2. Deal $(A,Q)$:
   - I: $A\mapsto N$.
   - II: $Q\mapsto B$.
   - Exactly one bet (II bets) $\Rightarrow$ I loses.
   - Payoff to I: $-1$.

3. Deal $(K,A)$:
   - I: $K\mapsto N$.
   - II: $A\mapsto B$.
   - II bets alone $\Rightarrow$ I loses.
   - Payoff to I: $-1$.

4. Deal $(K,Q)$:
   - I: $K\mapsto N$.
   - II: $Q\mapsto B$.
   - II bets alone $\Rightarrow$ I loses.
   - Payoff to I: $-1$.

5. Deal $(Q,A)$:
   - I: $Q\mapsto N$.
   - II: $A\mapsto B$.
   - II bets alone $\Rightarrow$ I loses.
   - Payoff to I: $-1$.

6. Deal $(Q,K)$:
   - I: $Q\mapsto N$.
   - II: $K\mapsto N$.
   - Both $N$ $\Rightarrow$ showdown, $K$ wins.
   - Payoff to I: $-1$.

Sum:
$$
(+1) + (-1)+(-1)+(-1)+(-1)+(-1) = -4.
$$

Average over 6 equally likely deals:
$$
A_{1,6} = -\frac{4}{6} = -\frac{2}{3}.
$$



#### Step 4: The Full Payoff Matrix

With the strategy ordering

$$
NNN,\ NNB,\ NBN,\ NBB,\ BNN,\ BNB,\ BBN,\ BBB,
$$

the payoff matrix to Player I is

$$
A=\frac{1}{6}
\begin{pmatrix}
 0 & -4 & -2 & -6 &  0 & -4 & -2 & -6\\
 4 &  0 & -1 & -5 &  1 & -3 & -4 & -8\\
 2 &  1 &  0 & -1 & -1 & -2 & -3 & -4\\
 6 &  5 &  1 &  0 &  0 & -1 & -5 & -6\\
 0 & -1 &  1 &  0 &  0 & -1 &  1 &  0\\
 4 &  3 &  2 &  1 &  1 &  0 & -1 & -2\\
 2 &  4 &  3 &  5 & -1 &  1 &  0 &  2\\
 6 &  8 &  4 &  6 &  0 &  2 & -2 &  0
\end{pmatrix}.
$$

Because the game is symmetric and zero–sum, Player II’s payoff matrix is $-A^T$ if we switch player roles, and the expected payoff to Player II is always $-u_1$.



#### What This Example Illustrates

This example makes precise the relationship between:

- **randomness** (the deal of cards),
- **strategies** (contingent betting rules),
- **payoff matrices** (expected value over feasible deals).

The payoff matrix is not indexed by card outcomes.  
It is indexed by strategies.

Card incompatibilities are handled at the probability level: impossible deals have probability $0$ and do not appear in the expectation.

### Dominated Strategies

Before analyzing best responses and security levels, we simplify games by eliminating strategies that are never rational to play.

#### Definition (Strict Dominance)

Let $A$ be the payoff matrix for a two-player zero–sum game.

A row $i$ is **strictly dominated** by row $i'$ if

$$
A_{i'j} > A_{ij}
\quad
\text{for all } j.
$$

That is, regardless of what Player II does, strategy $i'$ yields a strictly higher payoff than strategy $i$.

In this case, strategy $i$ can never be optimal and may be removed from the game.

Similarly, a column $j$ is strictly dominated by column $j'$ if

$$
A_{ij'} < A_{ij}
\quad
\text{for all } i.
$$

Since Player II minimizes Player I’s payoff, a dominated column always gives Player I a larger payoff and is therefore irrational for Player II.



#### Weak Dominance

Row $i$ is **weakly dominated** by row $i'$ if

$$
A_{i'j} \ge A_{ij}
\quad
\text{for all } j,
$$

with strict inequality for at least one $j$.

Weakly dominated strategies are never better and sometimes worse.

In zero–sum games, strictly dominated strategies may be safely eliminated without affecting equilibrium analysis. Weak dominance requires more care but often provides useful simplification.



### Example: Dominance in the Three-Card Poker Game

Recall that pure strategies are encoded as triples indicating actions with $(A,K,Q)$:

$$
NNN,\ NNB,\ NBN,\ NBB,\ BNN,\ BNB,\ BBN,\ BBB.
$$

Consider the strategy $NNN$, which never bets.

Now compare it with $BNN$, which differs only in betting with $A$.

For every possible opponent strategy:

- When holding $A$, betting strictly improves payoff (it either wins uncontested or doubles winnings at showdown).
- With $K$ and $Q$, both strategies behave identically.

Thus for every column $j$,

$$
A_{BNN,j} \ge A_{NNN,j},
$$

with strict inequality for some $j$.

Therefore:

> $NNN$ is weakly dominated by $BNN$.

Intuitively, refusing to bet with the highest card sacrifices value without ever providing protection.



Similarly, any strategy that fails to bet with $A$ is weakly dominated by the identical strategy that *does* bet with $A$.

Thus, rational play restricts attention to strategies that always bet with $A$.

This observation reduces the strategic space from eight pure strategies to four:

$$
BNN,\ BNB,\ BBN,\ BBB.
$$

The game becomes structurally simpler before any equilibrium analysis begins.



### Conceptual Role of Dominance

Dominated strategies represent actions that are inferior regardless of the opponent’s behavior.

Eliminating them:

- Reduces the size of the game,
- Clarifies the strategic structure,
- Sharpens best-response analysis.

In poker terms, a dominated strategy is a betting rule that leaves money on the table no matter how the opponent plays.

The study of equilibrium begins only after clearly irrational strategies are removed.

### Best Responses (Reduced Game)

After eliminating dominated strategies, each player has four remaining pure strategies:

$$
BNN,\ BNB,\ BBN,\ BBB.
$$

These are the strategies that always bet with $A$ and vary behavior with $K$ and $Q$.

Let $A$ now denote the reduced $4 \times 4$ payoff matrix obtained by restricting the original matrix to these strategies.

Rows correspond to Player I and columns to Player II in the order

$$
BNN,\ BNB,\ BBN,\ BBB.
$$

Suppose Player II chooses column $j$.

Player I’s optimal pure response is any row index $i$ that maximizes

$$
A_{ij}.
$$

Thus Player I seeks

$$
\max_i A_{ij}.
$$

Similarly, if Player I chooses row $i$, Player II seeks to minimize Player I’s payoff:

$$
\min_j A_{ij}.
$$

Because the game is zero–sum, the players optimize opposing objectives over the same matrix.



### Example: A Best Response in the Reduced Game

Suppose Player II adopts the strategy $BNN$ (bet only with $A$).

In the reduced matrix, the corresponding column is

$$
\frac{1}{6}
\begin{pmatrix}
0 \\
1 \\
-1 \\
0
\end{pmatrix}.
$$

The largest entry is $1/6$, achieved by the strategy $BNB$.

Thus, if Player II bets only with $A$, Player I’s best response is to bet with $A$ and $Q$, but not with $K$.

The strategic adjustment now concerns bluffing frequency rather than whether to bet with $A$.



## Maximin and Minimax (Reduced Game)

Best responses describe local reactions.  
We now evaluate global security levels in the reduced matrix.

### Maximin

Player I evaluates each row by its worst outcome:

$$
\min_j A_{ij}.
$$

Then selects

$$
\max_i \min_j A_{ij}.
$$

For the reduced matrix, the row minima (scaled by $1/6$) are:

$$
\begin{array}{c|c}
\text{Strategy} & \min_j A_{ij} \\
\hline
BNN & -\frac{1}{6} \\
BNB & -\frac{2}{6} \\
BBN & -\frac{1}{6} \\
BBB & -\frac{2}{6}
\end{array}
$$

Thus

$$
\max_i \min_j A_{ij}
=
-\frac{1}{6}.
$$



### Minimax

Player II evaluates each column by

$$
\max_i A_{ij},
$$

and chooses

$$
\min_j \max_i A_{ij}.
$$

For the reduced matrix, the smallest column maximum equals

$$
\frac{1}{6}.
$$

Thus

$$
\min_j \max_i A_{ij}
=
\frac{1}{6}.
$$



### Fundamental Inequality

For every finite matrix $A$,

$$
\max_i \min_j A_{ij}
\;\le\;
\min_j \max_i A_{ij}.
$$

In the reduced poker matrix,

$$
-\frac{1}{6}
<
\frac{1}{6}.
$$

The inequality is strict.



## Saddle Points

A pair $(i^*, j^*)$ is a **saddle point** if

$$
A_{i^* j} \le A_{i^* j^*} \le A_{i j^*}
\quad
\text{for all } i, j.
$$

Equivalently, the entry is:

- The smallest value in its row,
- The largest value in its column.

If a saddle point exists, then

$$
\max_i \min_j A_{ij}
=
\min_j \max_i A_{ij}.
$$



## Absence of a Saddle Point

In the reduced three-card poker game,

$$
\max_i \min_j A_{ij}
=
-\frac{1}{6}
\quad
<
\quad
\frac{1}{6}
=
\min_j \max_i A_{ij}.
$$

Therefore:

- No saddle point exists.
- No pure strategy stabilizes the game.
- Every deterministic betting rule (even after removing dominated ones) can be exploited.

The strategic tension now lies entirely in how frequently players bluff with $Q$ or value-bet with $K$.

This structural failure of pure strategies motivates enlarging the strategy space.

## V. Mixed Strategies

The failure of pure strategies in the reduced three-card poker game forces us to enlarge the strategic framework.

After eliminating dominated strategies, each player has four pure strategies:

$$
BNN,\ BNB,\ BBN,\ BBB,
$$

that is:

- Always bet with $A$,
- Choose whether to bet with $K$,
- Choose whether to bet with $Q$.

We saw that no saddle point exists in this $4 \times 4$ matrix.  
Every deterministic betting rule can be countered.



### Motivation

The structural problem is rigidity.

A pure strategy fixes a single row (or column) of the payoff matrix. Once this row is known, the opponent chooses the column that minimizes it. The player is exposed to their worst-case outcome.

To prevent exploitation, a player may deliberately introduce randomness.

Instead of choosing a single pure strategy, the player chooses a probability distribution over the four available strategies.

Randomization here is not ignorance. It is a strategic device. By mixing between pure strategies, a player can make the opponent indifferent among responses and eliminate systematic vulnerability.

This enlargement of the strategy space is the key conceptual step that allows equilibrium to exist.



### Application to the Three-Card Poker Game

In the reduced game, betting with $A$ is fixed.  
The strategic tension lies in how often to bet with $K$ and how often to bluff with $Q$.

For example:

- If Player I never bluffs with $Q$, Player II can safely fold weak hands.
- If Player I always bluffs with $Q$, Player II can call more aggressively.

Thus Player I may choose to bet with $Q$ only with some probability $p$.

Similarly, Player II may choose to call (equivalently, bet in the simultaneous model) with $K$ only with some probability $q$.

These probabilities become part of the strategic choice.



### Probability Simplex

Let

$$
S_1 = \{BNN,\ BNB,\ BBN,\ BBB\}
$$

be Player I’s reduced pure strategy set.

A **mixed strategy** for Player I is a probability vector

$$
x = (x_1,x_2,x_3,x_4)
$$

satisfying

$$
x_i \ge 0
\quad\text{and}\quad
\sum_{i=1}^4 x_i = 1.
$$

The set of all mixed strategies is the probability simplex

$$
\Delta_4
=
\left\{
x \in \mathbb{R}^4 :
x_i \ge 0,\;
\sum_{i=1}^4 x_i = 1
\right\}.
$$

Similarly, Player II’s mixed strategies form a simplex $\Delta_4$.

Each vertex of the simplex corresponds to a pure strategy.  
Interior points correspond to genuine randomization.



### Behavioral Interpretation

Although a mixed strategy is formally a probability distribution over the four complete betting plans, the structure of the game suggests a more natural interpretation.

Since betting with $A$ is fixed, a player’s behavior is fully described by two numbers:

- The probability of betting with $K$,
- The probability of betting with $Q$.

Thus the strategic space effectively becomes a rectangle inside the simplex, parameterized by bluffing and thin-value frequencies.

Mixed strategies therefore smooth the payoff landscape by averaging across rows of the reduced payoff matrix.

In the next subsection, we compute expected payoffs under mixed strategies and examine the structure that makes equilibrium possible.

### Expected Payoff

Let

$$
A \in \mathbb{R}^{m \times n}
$$

be the payoff matrix of a finite two–player zero–sum game.

If Player I chooses a mixed strategy

$$
x \in \Delta_m
$$

and Player II chooses

$$
y \in \Delta_n,
$$

the expected payoff to Player I is

$$
u(x,y) = x^T A y.
$$

Explicitly,

$$
u(x,y)
=
\sum_{i=1}^m \sum_{j=1}^n x_i A_{ij} y_j.
$$

Interpretation:

- Player I randomizes over rows according to $x$.
- Player II randomizes over columns according to $y$.
- The payoff is the weighted average of matrix entries.

Mixed strategies therefore transform a discrete payoff table into a continuous bilinear function defined on

$$
\Delta_m \times \Delta_n.
$$



## Application to the Three-Card Poker Game

After eliminating dominated strategies, each player has four pure strategies:

$$
BNN,\ BNB,\ BBN,\ BBB.
$$

Thus the reduced payoff matrix is

$$
A \in \mathbb{R}^{4 \times 4}.
$$

Each entry $A_{ij}$ already represents an expectation over the six equally likely card deals.



### Full Simplex Viewpoint (4–Dimensional)

A mixed strategy for Player I is

$$
x = (x_1,x_2,x_3,x_4) \in \Delta_4,
$$

where

- $x_1$ = probability of $BNN$,
- $x_2$ = probability of $BNB$,
- $x_3$ = probability of $BBN$,
- $x_4$ = probability of $BBB$.

Similarly,

$$
y = (y_1,y_2,y_3,y_4) \in \Delta_4
$$

for Player II.

The expected payoff is

$$
u(x,y) = x^T A y.
$$

This averages over:

1. Nature’s random deal (already built into each $A_{ij}$),
2. Player I’s strategic randomization,
3. Player II’s strategic randomization.

Thus the total expectation is layered:

- Card randomness inside each matrix entry,
- Strategic randomness across matrix entries.



### Behavioral Parameterization (2–Dimensional View)

Although formally each player chooses a point in $\Delta_4$, the structure of the game allows a simpler description.

Betting with $A$ is fixed in all nondominated strategies.

The only strategic freedom lies in:

- Whether to bet with $K$,
- Whether to bet with $Q$.

Thus Player I’s behavior can be described by two probabilities:

$$
\alpha = \Pr(\text{bet with } K),
$$

$$
\beta = \Pr(\text{bet with } Q).
$$

Similarly for Player II:

$$
\gamma = \Pr(\text{bet with } K),
$$

$$
\delta = \Pr(\text{bet with } Q).
$$

These four parameters determine a unique mixed strategy in $\Delta_4$.

For example, Player I’s simplex coordinates become

$$
\begin{aligned}
x_1 &= (1-\alpha)(1-\beta), \\
x_2 &= (1-\alpha)\beta, \\
x_3 &= \alpha(1-\beta), \\
x_4 &= \alpha\beta.
\end{aligned}
$$

Thus the 4–dimensional simplex collapses to a rectangle

$$
[0,1]^2
$$

parameterized by $(\alpha,\beta)$.

The expected payoff may therefore be written equivalently as

$$
u(\alpha,\beta;\gamma,\delta),
$$

a bilinear function of the bluffing and value-betting frequencies.



### Conceptual Perspective

There are two equivalent viewpoints:

1. **Strategic-form viewpoint**  
   Players choose probability vectors in $\Delta_4$, and payoffs are computed as $x^T A y$.

2. **Behavioral viewpoint**  
   Players randomize directly at decision points (with $K$ and $Q$), producing a lower-dimensional parameterization.

The first viewpoint emphasizes linear algebra and convex geometry.  
The second emphasizes poker interpretation.

Mathematically, they describe the same mixed strategies.
### Structural Properties

The expected payoff function

$$
u(x,y) = x^T A y
$$

has several key structural properties.

We interpret these properties both in the full simplex formulation
$$
x,y \in \Delta_4
$$
and in the behavioral parameterization
$$
(\alpha,\beta), (\gamma,\delta) \in [0,1]^2
$$
for the three-card poker game.



#### Bilinearity

The payoff function is linear in each argument separately.

Fix $y$. Then

$$
u(x,y)
=
x^T (A y)
$$

is linear in $x$.

Fix $x$. Then

$$
u(x,y)
=
(A^T x)^T y
$$

is linear in $y$.

Thus the function is **bilinear**.



##### Interpretation in the Reduced Poker Game

In the simplex formulation:

- Player I mixes between the four strategies
  $$
  BNN,\ BNB,\ BBN,\ BBB.
  $$
- Increasing the weight on one row shifts the payoff linearly.

In the behavioral formulation:

Let

$$
\alpha = \Pr(\text{bet with } K), 
\qquad
\beta = \Pr(\text{bet with } Q).
$$

Then $u$ becomes a bilinear function

$$
u(\alpha,\beta;\gamma,\delta).
$$

If Player I increases $\beta$ (bluff frequency), the expected payoff changes linearly in $\beta$ for fixed opponent behavior.

There are no nonlinear strategic effects at this stage.  
The complexity arises from strategic interaction, not from curvature of the payoff function.



#### Convexity of Strategy Sets

The mixed strategy set

$$
\Delta_4
$$

is a convex subset of $\mathbb{R}^4$.

If

$$
x, x' \in \Delta_4
$$

and

$$
0 \le \lambda \le 1,
$$

then

$$
\lambda x + (1-\lambda)x' \in \Delta_4.
$$

Thus mixing between mixed strategies remains valid.



##### Behavioral Interpretation

Under the behavioral parameterization, the strategy space reduces to

$$
[0,1]^2.
$$

If Player I uses bluff frequency $\beta_1$ and another strategy uses $\beta_2$, then any convex combination

$$
\lambda \beta_1 + (1-\lambda)\beta_2
$$

is again a valid bluff frequency.

Thus the strategic space contains no gaps.

Convexity eliminates the discrete rigidity that caused pure strategies to fail.



#### Continuity

Because $u(x,y)$ is linear in each argument, it is continuous on

$$
\Delta_4 \times \Delta_4.
$$

Equivalently, in behavioral form,

$$
u(\alpha,\beta;\gamma,\delta)
$$

is continuous on

$$
[0,1]^2 \times [0,1]^2.
$$

Small changes in bluffing or value-betting frequencies produce small changes in expected payoff.



### Structural Summary

The enlargement from four discrete betting rules to the convex set $\Delta_4$ (or equivalently $[0,1]^2$) produces:

- A convex strategic domain,
- A bilinear payoff function,
- A continuous optimization problem.

These structural properties are precisely what fail in pure strategies.

Together they create the mathematical environment in which equilibrium can be guaranteed.

We now formalize this guarantee.

### Solving the Three–Card Game in Behavioral Form

Because betting with $A$ strictly dominates checking with $A$, we restrict attention to strategies determined by two parameters:

For Player I:
$$
\alpha = \Pr(\text{bet with } K),
\qquad
\beta = \Pr(\text{bet with } Q).
$$

For Player II:
$$
\gamma = \Pr(\text{bet with } K),
\qquad
\delta = \Pr(\text{bet with } Q).
$$

Thus each player’s strategy lies in $[0,1]^2$.

We now compute equilibrium frequencies.



## Step 1: Expected Payoff with $Q$ (Bluffing)

Suppose Player I holds $Q$.

The opponent holds $A$ or $K$, each with probability $1/2$.

Player II always bets with $A$, and bets with $K$ with probability $\gamma$.

Thus Player II bets with probability

$$
\frac{1 + \gamma}{2}.
$$

If Player I bets with $Q$:

- If Player II does not bet, Player I wins $+1$.
- If Player II bets, both bet and $Q$ loses $-2$.

Therefore

$$
EV_Q(\text{Bet})
=
1 \cdot \left(1 - \frac{1+\gamma}{2}\right)
+
(-2) \cdot \frac{1+\gamma}{2}.
$$

Simplify:

$$
EV_Q(\text{Bet})
=
\frac{1-\gamma}{2}
-
\frac{2(1+\gamma)}{2}
=
\frac{1-\gamma - 2 - 2\gamma}{2}
=
\frac{-1 - 3\gamma}{2}.
$$

If Player I does not bet with $Q$, then both check unless Player II bets:

- If Player II bets, Player I loses $-1$.
- If Player II does not bet, both check and $Q$ loses $-1$.

Thus checking yields

$$
EV_Q(\text{Check}) = -1.
$$

Indifference requires

$$
EV_Q(\text{Bet}) = EV_Q(\text{Check}).
$$

So

$$
\frac{-1 - 3\gamma}{2} = -1.
$$

Multiply by 2:

$$
-1 - 3\gamma = -2.
$$

Thus

$$
3\gamma = 1
\quad\Rightarrow\quad
\gamma = \frac{1}{3}.
$$

So in equilibrium:

> Player II must bet with $K$ with probability $\frac{1}{3}$.



## Step 2: Expected Payoff with $K$

Now suppose Player I holds $K$.

The opponent holds $A$ or $Q$, each with probability $1/2$.

Player II bets with $A$ always, and with $Q$ with probability $\delta$.

Thus Player II bets with probability

$$
\frac{1 + \delta}{2}.
$$

If Player I bets with $K$:

- If Player II does not bet, Player I wins $+1$.
- If Player II bets:
  - against $A$, $K$ loses $-2$,
  - against $Q$, $K$ wins $+2$.

Conditional on betting by Player II, the opponent holds $A$ with probability
$$
\frac{1}{1+\delta}
$$
and $Q$ with probability
$$
\frac{\delta}{1+\delta}.
$$

Thus

$$
EV_K(\text{Bet})
=
1 \cdot \left(1 - \frac{1+\delta}{2}\right)
+
\left(
\frac{\delta}{1+\delta} \cdot 2
+
\frac{1}{1+\delta} \cdot (-2)
\right)
\cdot
\frac{1+\delta}{2}.
$$

Simplify inside:

$$
\frac{\delta - 1}{1+\delta} \cdot 2.
$$

Multiplying by $(1+\delta)/2$ gives

$$
\delta - 1.
$$

So

$$
EV_K(\text{Bet})
=
\frac{1-\delta}{2}
+
(\delta - 1)
=
\frac{-1 + \delta}{2}.
$$

If Player I checks with $K$:

- If Player II bets, Player I loses $-1$.
- If Player II checks, both check and $K$ wins $+1$.

Thus

$$
EV_K(\text{Check})
=
\frac{1-\delta}{2}.
$$

Indifference requires

$$
\frac{-1 + \delta}{2}
=
\frac{1-\delta}{2}.
$$

Multiply by 2:

$$
-1 + \delta = 1 - \delta.
$$

So

$$
2\delta = 2
\quad\Rightarrow\quad
\delta = 1.
$$

Thus Player II must bet with $Q$ with probability 1.

But betting with $Q$ is strictly worse than checking (it loses at showdown and loses double when both bet). Therefore $\delta = 0$ is enforced by dominance.

So Player I must not be indifferent on $K$.

Instead, optimal play forces

$$
\alpha = 0.
$$

Thus Player I never bets with $K$.



## Step 3: Symmetry

By symmetry, Player II also must bluff with $Q$ at frequency

$$
\beta = \frac{1}{3},
$$

and never bet with $K$.



## Equilibrium

The symmetric equilibrium is:

$$
\Pr(\text{Bet}\mid A)=1,
$$
$$
\Pr(\text{Bet}\mid K)=0,
$$
$$
\Pr(\text{Bet}\mid Q)=\frac{1}{3}.
$$

Each player bluffs with $Q$ one-third of the time.



## Value of the Game

Under these frequencies, the expected payoff is

$$
v = 0.
$$

Neither player has an advantage before cards are dealt.



## Interpretation

- Bluffing must occur, but not too often.
- The frequency $\frac{1}{3}$ arises from forcing the opponent’s $K$ to be indifferent between betting and checking.
- Randomization stabilizes the game.

The instability observed under pure strategies disappears once players randomize at the correct frequencies.

### Why General–Sum Games Require a Different Equilibrium Concept

In a general–sum game, the sum

$$
u_1(s_1,s_2) + u_2(s_1,s_2)
$$

varies across strategy profiles.

Players may have:

- Partially aligned interests,
- Mutually beneficial outcomes,
- Strategic tradeoffs that are not purely adversarial.

In such games:

- There is no single value $v$ that both players can force.
- One player’s maximization problem does not coincide with the other’s minimization problem.
- Indifference conditions arise differently, if at all.

The correct equilibrium concept must capture mutual best responses rather than strict opposition.

### Preview: Nash Equilibrium

The natural generalization of minimax equilibrium is the concept of **Nash equilibrium**.

A strategy profile $(x^*, y^*)$ is a Nash equilibrium if:

- $x^*$ is a best response to $y^*$,
- $y^*$ is a best response to $x^*$.

Formally,

$$
u_1(x^*, y^*) \ge u_1(x, y^*) \quad \text{for all } x,
$$

$$
u_2(x^*, y^*) \ge u_2(x^*, y) \quad \text{for all } y.
$$

In zero–sum games, Nash equilibrium coincides with minimax equilibrium. The two concepts agree.

In general–sum games, however, Nash equilibrium replaces minimax as the central solution concept. It does not rely on a single game value, but on mutual optimality.

The transition from minimax to Nash marks a conceptual shift:

- From pure conflict to mixed incentives,
- From a single scalar value to a vector of payoffs,
- From minimization–maximization symmetry to mutual best responses.

The tools developed in this chapter — mixed strategies, convexity, and indifference — remain central. But the structure of strategic interaction broadens.

In the next chapter, we begin that transition.

## Beyond Poker: Mixed Strategies in Sports Analytics

Mixed strategies are not confined to gambling games. They arise naturally in competitive environments where opponents actively adapt.

A canonical example comes from American football play-calling.

### A Simplified Run–Pass Model

Consider a short-yardage situation. The offense must choose between:

- **Run**
- **Pass**

The defense must choose between:

- **Defend Run**
- **Defend Pass**

Assume the following payoff matrix represents the expected yards gained by the offense:

$$
A =
\begin{pmatrix}
4 & 1 \\
2 & 5
\end{pmatrix}
$$

Rows correspond to the offense (Run, Pass).  
Columns correspond to the defense (Defend Run, Defend Pass).

Interpretation:

- If the offense runs and the defense defends run, the offense gains 4 yards.
- If the offense runs and the defense defends pass, the offense gains 1 yard.
- If the offense passes and the defense defends run, the offense gains 2 yards.
- If the offense passes and the defense defends pass, the offense gains 5 yards.

This is a zero–sum model if we interpret defensive payoff as the negative of offensive yardage.

### No Pure Strategy Equilibrium

If the offense always runs, the defense will defend run.  
If the offense always passes, the defense will defend pass.  
Each pure strategy can be exploited.

Thus we look for mixed strategies.

Let:

- $p$ = probability the offense runs,
- $q$ = probability the defense defends run.

### Indifference Conditions

If the defense chooses $q$, the offense’s expected payoff from running is

$$
EV_{\text{Run}} = 4q + 1(1-q) = 1 + 3q.
$$

The payoff from passing is

$$
EV_{\text{Pass}} = 2q + 5(1-q) = 5 - 3q.
$$

At equilibrium, the offense must be indifferent:

$$
1 + 3q = 5 - 3q
\quad\Longrightarrow\quad
6q = 4
\quad\Longrightarrow\quad
q = \frac{2}{3}.
$$

Thus the defense must defend the run $2/3$ of the time.

Now solve for the offense’s mixing probability.

If the offense chooses $p$, the defense’s expected loss from defending run is

$$
L_{\text{Def Run}} = 4p + 2(1-p) = 2 + 2p,
$$

and from defending pass:

$$
L_{\text{Def Pass}} = 1p + 5(1-p) = 5 - 4p.
$$

Indifference requires

$$
2 + 2p = 5 - 4p
\quad\Longrightarrow\quad
6p = 3
\quad\Longrightarrow\quad
p = \frac{1}{2}.
$$

### Equilibrium Interpretation

The equilibrium mixed strategies are:

- Offense runs with probability $1/2$.
- Defense defends run with probability $2/3$.

Under these probabilities:

- Neither side can improve expected yardage by deviating.
- Each side makes the other indifferent between their available actions.

The expected value of the play is

$$
v = 1 + 3\left(\frac{2}{3}\right) = 3.
$$

Thus, under optimal play, the offense gains 3 expected yards.

### Strategic Insight

The offense does not run 50% of the time because running is intrinsically optimal.  
It runs 50% of the time because doing so prevents the defense from exploiting predictability.

Similarly, the defense does not defend the run 2/3 of the time because that action is intrinsically superior.  
It does so to eliminate the offense’s ability to gain by shifting play selection.

Mixed strategies emerge as a structural necessity of competition, not as indecision.

The same logic governs:

- Pitch selection in baseball,
- Shot distribution in basketball,
- Serve placement in tennis,
- Penalty kick direction in soccer.

Whenever opponents adapt strategically, equilibrium requires carefully calibrated randomness.


## Liberal Arts Sidebar  
### War, Games, and Human Cost  

**Lens:** History / Ethics  
**Theme:** Game theory’s military development and the moral risks of zero–sum framing  

Modern game theory is often introduced through poker or sports. Historically, however, its most influential applications emerged in the shadow of nuclear war.

In 1944, :contentReference[oaicite:0]{index=0} and :contentReference[oaicite:1]{index=1} published *Theory of Games and Economic Behavior*, formalizing strategic reasoning for adversarial settings. Within a decade, these ideas were being developed aggressively at the :contentReference[oaicite:2]{index=2}, where mathematicians, economists, and physicists were tasked with modeling nuclear deterrence.

### Cold War Strategy and Zero–Sum Models

The central strategic problem of the early Cold War was deterrence: how could two nuclear powers prevent one another from initiating a first strike?

Analysts modeled this as a two-player zero–sum game:

- Player I: the United States  
- Player II: the Soviet Union  

Each could choose levels of armament, targeting doctrines, or launch postures. Payoffs represented strategic advantage or vulnerability.

A simplified version of the logic was:

- If one side disarmed, it risked catastrophic loss.
- If one side struck first, the other might retaliate.
- If both retained second-strike capability, mutual destruction deterred aggression.

The resulting equilibrium concept — later described as “mutually assured destruction” — resembled minimax reasoning: each side sought to minimize the worst possible outcome imposed by the other.

Game-theoretic reasoning also shaped escalation theory. :contentReference[oaicite:3]{index=3} introduced models of credible commitment, brinkmanship, and strategic signaling. He emphasized that power often lay not in maximizing immediate payoff, but in manipulating an opponent’s expectations.

Nuclear strategy was therefore not merely about weapons. It was about incentives, beliefs, and equilibrium stability.

### Formal Models in Practice

Several concrete strategic tools emerged:

- **Second-strike capability models:** ensuring survivable retaliatory forces to maintain deterrence.
- **Arms race models:** analyzing whether increasing weapon stockpiles improved security or reduced it.
- **Crisis bargaining models:** predicting behavior during standoffs such as the Cuban Missile Crisis.

In each case, the underlying structure often assumed strict opposition:

$$
u_1 + u_2 = 0.
$$

One state’s strategic gain was modeled as the other’s loss.

### Contemporary Military Planning

Game-theoretic methods remain embedded in modern defense planning.

Applications include:

- Missile defense allocation models  
- Cybersecurity response games  
- Drone swarm and autonomous systems coordination  
- Strategic simulations of regional conflicts  

Military planners use computational game-theoretic models to evaluate adversarial adaptation. War games — both tabletop and computational — simulate multi-stage strategic interaction under uncertainty.

Today’s models are often more complex than early Cold War formulations. They incorporate incomplete information, signaling, repeated interaction, and probabilistic outcomes. Yet the adversarial core frequently retains a zero–sum structure.

### The Ethical Tension

Mathematically, zero–sum framing offers clarity. It sharpens incentives and produces equilibrium predictions. But the abstraction can obscure critical dimensions.

When nuclear strategy is modeled as a payoff matrix, “loss” may represent millions of lives. When cyberwarfare is modeled as optimization, civilian infrastructure may become a variable.

A zero–sum lens highlights conflict but suppresses shared vulnerability. During the Cold War, arms control agreements such as the Strategic Arms Limitation Talks (SALT) reflected recognition that some strategic interactions were not purely zero–sum. Cooperation could reduce mutual risk.

The mathematics of zero–sum games is precise. The moral landscape it describes is not.

### A Reflective Question

Game theory helps clarify strategic stability. But it also shapes how conflicts are conceptualized.

> When does zero–sum reasoning illuminate reality?  
> When does it risk narrowing moral imagination or obscuring the possibility of shared survival?

The power of mathematical modeling lies not only in what it reveals, but in what it leaves out.

## Homework Problems

### Conceptual Foundations
## Homework Problems

### I. Structural and Proof Problems

1. **Constant–Sum Games**

   (a) Prove that every constant–sum two–player game is strategically equivalent to a zero–sum game.

   (b) Show explicitly how to transform the payoff matrix
   $$
   A =
   \begin{pmatrix}
   2 & 5 \\
   7 & 1
   \end{pmatrix}
   $$
   into a zero–sum representation.



2. **Saddle Points and Mixed Strategies**

   Prove that if a saddle point exists in a finite two–player zero–sum game, then the corresponding pure strategy pair remains an equilibrium after allowing mixed strategies.



3. **Maximin Inequality**

   Prove that for every real matrix $A$,
   $$
   \max_i \min_j A_{ij}
   \;\le\;
   \min_j \max_i A_{ij}.
   $$

   (a) When does equality hold?  
   (b) Show that equality holds if and only if a saddle point exists.



4. **Indifference Principle (Proof)**

   Let $(x^*, y^*)$ be optimal mixed strategies in a finite zero–sum game with value $v$.

   Prove that if $x_i^* > 0$, then
   $$
   (A y^*)_i = v.
   $$

   (Hint: argue by contradiction.)



5. **Uniqueness of the Game Value**

   Prove that although optimal mixed strategies may not be unique, the value $v$ of a finite zero–sum game is unique.



### II. Computational Problems (Matrix Games)

6. **A $2\times2$ Bluffing Game**

   Consider the payoff matrix
   $$
   A =
   \begin{pmatrix}
   2 & -1 \\
   -2 & 1
   \end{pmatrix}.
   $$

   (a) Show that no pure saddle point exists.  
   (b) Compute the optimal mixed strategies.  
   (c) Verify the indifference principle directly.  
   (d) Compute the value of the game.



7. **Dominated Strategies**

   Consider the matrix
   $$
   A =
   \begin{pmatrix}
   3 & 1 & 2 \\
   2 & 0 & 1 \\
   4 & 2 & 3
   \end{pmatrix}.
   $$

   (a) Identify all strictly or weakly dominated strategies.  
   (b) Reduce the matrix.  
   (c) Determine whether a saddle point exists.



### III. The Three–Card Poker Game

In this section, refer to the simultaneous three-card poker model from the chapter:

- Deck $\{A,K,Q\}$,
- Both players simultaneously choose Bet or Not,
- Payoffs:
  - Both check → high card wins $+1$,
  - One bets → bettor wins $+1$,
  - Both bet → high card wins $+2$.

After eliminating dominated strategies, each player has four pure strategies:
$$
BNN,\ BNB,\ BBN,\ BBB.
$$



8. **Reduced Payoff Matrix**

   (a) Write down the reduced $4 \times 4$ payoff matrix explicitly.

   (b) Verify directly that no saddle point exists.

   (c) Compute
   $$
   \max_i \min_j A_{ij}
   \quad\text{and}\quad
   \min_j \max_i A_{ij}.
   $$



9. **Behavioral Parameterization**

   Let
   $$
   \alpha = \Pr(\text{bet with } K), 
   \qquad
   \beta = \Pr(\text{bet with } Q).
   $$

   (a) Derive the expected payoff
   $$
   u(\alpha,\beta;\gamma,\delta)
   $$
   in terms of the opponent’s parameters $(\gamma,\delta)$.

   (b) Show explicitly that $u$ is bilinear.



10. **Solving the Poker Equilibrium**

   Using the behavioral formulation:

   (a) Derive the indifference condition that determines the equilibrium bluffing frequency.

   (b) Solve explicitly for the equilibrium frequencies.

   (c) Compute the value of the game.

   (d) Explain in words why bluffing must occur but not too frequently.



11. **Stability**

   Suppose Player I bluffs with $Q$ at frequency $\beta > \beta^*$.

   (a) Show that Player II can exploit this deviation.

   (b) Compute Player I’s loss from over-bluffing.



### IV. Geometry and Structure

12. **Convexity**

   Let $f(x,y) = x^T A y$.

   (a) Prove that $f$ is linear in $x$ for fixed $y$.

   (b) Show that
   $$
   \min_y f(x,y)
   $$
   is a concave function of $x$.

   (c) Explain why this concavity is relevant for equilibrium existence.



13. **Degenerate Equilibria**

   Construct a $2\times2$ zero–sum matrix game with infinitely many mixed equilibria.

   Explain why this does not contradict the minimax theorem.



### V. Conceptual and Modeling Questions

14. **When Is Randomization Necessary?**

   Provide a precise mathematical condition under which a finite two–player zero–sum game requires mixed strategies (i.e., has no pure saddle point).

   Illustrate your answer with a poker-inspired matrix.



15. **Zero–Sum Modeling**

   Is cash-game poker strictly zero–sum? What about tournament poker?  How do these variations differ?

   Consider:

   - Rake,
   - Entry fees,
   - External rewards.

   Provide a mathematical argument analyzing whether the zero–sum assumption holds.

## Extension Activity

Extend the chapter’s main tool using a small simulation or a short written reflection connecting poker to another domain.

